{
  "Large Language Models": [
    [
      {
        "id": "2503.10639",
        "title": "GoT: Unleashing Reasoning Capability of Multimodal Large Language Model for Visual Generation and Editing",
        "url": "http://arxiv.org/abs/2503.10639",
        "update_date": "2025-03-13",
        "first_author": "Rongyao Fang",
        "authors": "Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Shilin Yan, Hao Tian, Xingyu Zeng, Rui Zhao, Jifeng Dai, Xihui Liu, Hongsheng Li",
        "category": "cs.CV",
        "abstract": "Current image generation and editing methods primarily process textual prompts as direct inputs without reasoning about visual composition and explicit operations. We present Generation Chain-of-Thought (GoT), a novel paradigm that enables generation and editing through an explicit language reasoning process before outputting images. This approach transforms conventional text-to-image generation and editing into a reasoning-guided framework that analyzes semantic relationships and spatial arrangements. We define the formulation of GoT and construct large-scale GoT datasets containing over 9M samples with detailed reasoning chains capturing semantic-spatial relationships. To leverage the advantages of GoT, we implement a unified framework that integrates Qwen2.5-VL for reasoning chain generation with an end-to-end diffusion model enhanced by our novel Semantic-Spatial Guidance Module. Experiments show our GoT framework achieves excellent performance on both generation and editing tasks, with significant improvements over baselines. Additionally, our approach enables interactive visual generation, allowing users to explicitly modify reasoning steps for precise image adjustments. GoT pioneers a new direction for reasoning-driven visual generation and editing, producing images that better align with human intent. To facilitate future research, we make our datasets, code, and pretrained models publicly available at https://github.com/rongyaofang/GoT.",
        "comments": "Dataset and models are released in https://github.com/rongyaofang/GoT"
      },
      {
        "id": "2503.10635",
        "title": "A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1",
        "url": "http://arxiv.org/abs/2503.10635",
        "update_date": "2025-03-13",
        "first_author": "Zhaoyi Li",
        "authors": "Zhaoyi Li, Xiaohan Zhao, Dong-Dong Wu, Jiacheng Cui, Zhiqiang Shen",
        "category": "cs.CV",
        "abstract": "Despite promising performance on open-source large vision-language models (LVLMs), transfer-based targeted attacks often fail against black-box commercial LVLMs. Analyzing failed adversarial perturbations reveals that the learned perturbations typically originate from a uniform distribution and lack clear semantic details, resulting in unintended responses. This critical absence of semantic information leads commercial LVLMs to either ignore the perturbation entirely or misinterpret its embedded semantics, thereby causing the attack to fail. To overcome these issues, we notice that identifying core semantic objects is a key objective for models trained with various datasets and methodologies. This insight motivates our approach that refines semantic clarity by encoding explicit semantic details within local regions, thus ensuring interoperability and capturing finer-grained features, and by concentrating modifications on semantically rich areas rather than applying them uniformly. To achieve this, we propose a simple yet highly effective solution: at each optimization step, the adversarial image is cropped randomly by a controlled aspect ratio and scale, resized, and then aligned with the target image in the embedding space. Experimental results confirm our hypothesis. Our adversarial examples crafted with local-aggregated perturbations focused on crucial regions exhibit surprisingly good transferability to commercial LVLMs, including GPT-4.5, GPT-4o, Gemini-2.0-flash, Claude-3.5-sonnet, Claude-3.7-sonnet, and even reasoning models like o1, Claude-3.7-thinking and Gemini-2.0-flash-thinking. Our approach achieves success rates exceeding 90% on GPT-4.5, 4o, and o1, significantly outperforming all prior state-of-the-art attack methods. Our optimized adversarial examples under different configurations and training code are available at https://github.com/VILA-Lab/M-Attack.",
        "comments": "Code at: https://github.com/VILA-Lab/M-Attack"
      },
      {
        "id": "2503.10631",
        "title": "HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model",
        "url": "http://arxiv.org/abs/2503.10631",
        "update_date": "2025-03-13",
        "first_author": "Jiaming Liu",
        "authors": "Jiaming Liu, Hao Chen, Pengju An, Zhuoyang Liu, Renrui Zhang, Chenyang Gu, Xiaoqi Li, Ziyu Guo, Sixiang Chen, Mengzhen Liu, Chengkai Hou, Mengdi Zhao, KC alex Zhou, Pheng-Ann Heng, Shanghang Zhang",
        "category": "cs.CV",
        "abstract": "Recent advancements in vision-language models (VLMs) for common-sense reasoning have led to the development of vision-language-action (VLA) models, enabling robots to perform generalized manipulation. Although existing autoregressive VLA methods leverage large-scale pretrained knowledge, they disrupt the continuity of actions. Meanwhile, some VLA methods incorporate an additional diffusion head to predict continuous actions, relying solely on VLM-extracted features, which limits their reasoning capabilities. In this paper, we introduce HybridVLA, a unified framework that seamlessly integrates the strengths of both autoregressive and diffusion policies within a single large language model, rather than simply connecting them. To bridge the generation gap, a collaborative training recipe is proposed that injects the diffusion modeling directly into the next-token prediction. With this recipe, we find that these two forms of action prediction not only reinforce each other but also exhibit varying performance across different tasks. Therefore, we design a collaborative action ensemble mechanism that adaptively fuses these two predictions, leading to more robust control. In experiments, HybridVLA outperforms previous state-of-the-art VLA methods across various simulation and real-world tasks, including both single-arm and dual-arm robots, while demonstrating stable manipulation in previously unseen configurations.",
        "comments": ""
      },
      {
        "id": "2503.10630",
        "title": "UniGoal: Towards Universal Zero-shot Goal-oriented Navigation",
        "url": "http://arxiv.org/abs/2503.10630",
        "update_date": "2025-03-13",
        "first_author": "Hang Yin",
        "authors": "Hang Yin, Xiuwei Xu, Lingqing Zhao, Ziwei Wang, Jie Zhou, Jiwen Lu",
        "category": "cs.CV",
        "abstract": "In this paper, we propose a general framework for universal zero-shot goal-oriented navigation. Existing zero-shot methods build inference framework upon large language models (LLM) for specific tasks, which differs a lot in overall pipeline and fails to generalize across different types of goal. Towards the aim of universal zero-shot navigation, we propose a uniform graph representation to unify different goals, including object category, instance image and text description. We also convert the observation of agent into an online maintained scene graph. With this consistent scene and goal representation, we preserve most structural information compared with pure text and are able to leverage LLM for explicit graph-based reasoning. Specifically, we conduct graph matching between the scene graph and goal graph at each time instant and propose different strategies to generate long-term goal of exploration according to different matching states. The agent first iteratively searches subgraph of goal when zero-matched. With partial matching, the agent then utilizes coordinate projection and anchor pair alignment to infer the goal location. Finally scene graph correction and goal verification are applied for perfect matching. We also present a blacklist mechanism to enable robust switch between stages. Extensive experiments on several benchmarks show that our UniGoal achieves state-of-the-art zero-shot performance on three studied navigation tasks with a single model, even outperforming task-specific zero-shot methods and supervised universal methods.",
        "comments": "Accepted to CVPR 2025"
      },
      {
        "id": "2503.10622",
        "title": "Transformers without Normalization",
        "url": "http://arxiv.org/abs/2503.10622",
        "update_date": "2025-03-13",
        "first_author": "Jiachen Zhu",
        "authors": "Jiachen Zhu, Xinlei Chen, Kaiming He, Yann LeCun, Zhuang Liu",
        "category": "cs.LG",
        "abstract": "Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique. We introduce Dynamic Tanh (DyT), an element-wise operation $DyT($x$) = \\tanh(\\alpha $x$)$, as a drop-in replacement for normalization layers in Transformers. DyT is inspired by the observation that layer normalization in Transformers often produces tanh-like, $S$-shaped input-output mappings. By incorporating DyT, Transformers without normalization can match or exceed the performance of their normalized counterparts, mostly without hyperparameter tuning. We validate the effectiveness of Transformers with DyT across diverse settings, ranging from recognition to generation, supervised to self-supervised learning, and computer vision to language models. These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep networks.",
        "comments": "CVPR 2025; Project page: https://jiachenzhu.github.io/DyT/"
      },
      {
        "id": "2503.10619",
        "title": "Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search",
        "url": "http://arxiv.org/abs/2503.10619",
        "update_date": "2025-03-13",
        "first_author": "Andy Zhou",
        "authors": "Andy Zhou",
        "category": "cs.AI",
        "abstract": "We introduce Siege, a multi-turn adversarial framework that models the gradual erosion of Large Language Model (LLM) safety through a tree search perspective. Unlike single-turn jailbreaks that rely on one meticulously engineered prompt, Siege expands the conversation at each turn in a breadth-first fashion, branching out multiple adversarial prompts that exploit partial compliance from previous responses. By tracking these incremental policy leaks and re-injecting them into subsequent queries, Siege reveals how minor concessions can accumulate into fully disallowed outputs. Evaluations on the JailbreakBench dataset show that Siege achieves a 100% success rate on GPT-3.5-turbo and 97% on GPT-4 in a single multi-turn run, using fewer queries than baselines such as Crescendo or GOAT. This tree search methodology offers an in-depth view of how model safeguards degrade over successive dialogue turns, underscoring the urgency of robust multi-turn testing procedures for language models.",
        "comments": "Accepted to ICLR 2025 Trustworthy LLM"
      },
      {
        "id": "2503.10620",
        "title": "From TOWER to SPIRE: Adding the Speech Modality to a Text-Only LLM",
        "url": "http://arxiv.org/abs/2503.10620",
        "update_date": "2025-03-13",
        "first_author": "Kshitij Ambilduke",
        "authors": "Kshitij Ambilduke, Ben Peters, Sonal Sannigrahi, Anil Keshwani, Tsz Kin Lam, Bruno Martins, Marcely Zanon Boito, André F. T. Martins",
        "category": "cs.CL",
        "abstract": "Large language models (LLMs) have shown remarkable performance and generalization capabilities across multiple languages and tasks, making them very attractive targets for multi-modality integration (e.g., images or speech). In this work, we extend an existing LLM to the speech modality via speech discretization and continued pre-training. In particular, we are interested in multilingual LLMs, such as TOWER, as their pre-training setting allows us to treat discretized speech input as an additional translation language. The resulting open-source model, SPIRE, is able to transcribe and translate English speech input while maintaining TOWER's original performance on translation-related tasks, showcasing that discretized speech input integration as an additional language is feasible during LLM adaptation. We make our code and models available to the community.",
        "comments": ""
      },
      {
        "id": "2503.10617",
        "title": "Compositional Subspace Representation Fine-tuning for Adaptive Large Language Models",
        "url": "http://arxiv.org/abs/2503.10617",
        "update_date": "2025-03-13",
        "first_author": "Andy Zhou",
        "authors": "Andy Zhou",
        "category": "cs.CL",
        "abstract": "Adapting large language models to multiple tasks can cause cross-skill interference, where improvements for one skill degrade another. While methods such as LoRA impose orthogonality constraints at the weight level, they do not fully address interference in hidden-state representations. We propose Compositional Subspace Representation Fine-tuning (CS-ReFT), a novel representation-based approach that learns multiple orthonormal subspace transformations, each specializing in a distinct skill, and composes them via a lightweight router. By isolating these subspace edits in the hidden state, rather than weight matrices, CS-ReFT prevents cross-task conflicts more effectively. On the AlpacaEval benchmark, applying CS-ReFT to Llama-2-7B achieves a 93.94% win rate, surpassing GPT-3.5 Turbo (86.30%) while requiring only 0.0098% of model parameters. These findings show that specialized representation edits, composed via a simple router, significantly enhance multi-task instruction following with minimal overhead.",
        "comments": "Accepted to ICLR 2025 SCOPE"
      },
      {
        "id": "2503.10615",
        "title": "R1-Onevision: Advancing Generalized Multimodal Reasoning through Cross-Modal Formalization",
        "url": "http://arxiv.org/abs/2503.10615",
        "update_date": "2025-03-13",
        "first_author": "Yi Yang",
        "authors": "Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, Wei Chen",
        "category": "cs.CV",
        "abstract": "Large Language Models have demonstrated remarkable reasoning capability in complex textual tasks. However, multimodal reasoning, which requires integrating visual and textual information, remains a significant challenge. Existing visual-language models often struggle to effectively analyze and reason visual content, resulting in suboptimal performance on complex reasoning tasks. Moreover, the absence of comprehensive benchmarks hinders the accurate assessment of multimodal reasoning capabilities. In this paper, we introduce R1-Onevision, a multimodal reasoning model designed to bridge the gap between visual perception and deep reasoning. To achieve this, we propose a cross-modal reasoning pipeline that transforms images into formal textural representations, enabling precise language-based reasoning. Leveraging this pipeline, we construct the R1-Onevision dataset which provides detailed, step-by-step multimodal reasoning annotations across diverse domains. We further develop the R1-Onevision model through supervised fine-tuning and reinforcement learning to cultivate advanced reasoning and robust generalization abilities. To comprehensively evaluate multimodal reasoning performance across different grades, we introduce R1-Onevision-Bench, a benchmark aligned with human educational stages, covering exams from junior high school to university and beyond. Experimental results show that R1-Onevision achieves state-of-the-art performance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple challenging multimodal reasoning benchmarks.",
        "comments": "Code and Model: https://github.com/Fancy-MLLM/R1-onevision"
      },
      {
        "id": "2503.10613",
        "title": "CoSTA$\\ast$: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing",
        "url": "http://arxiv.org/abs/2503.10613",
        "update_date": "2025-03-13",
        "first_author": "Advait Gupta",
        "authors": "Advait Gupta, NandaKiran Velaga, Dang Nguyen, Tianyi Zhou",
        "category": "cs.CV",
        "abstract": "Text-to-image models like stable diffusion and DALLE-3 still struggle with multi-turn image editing. We decompose such a task as an agentic workflow (path) of tool use that addresses a sequence of subtasks by AI tools of varying costs. Conventional search algorithms require expensive exploration to find tool paths. While large language models (LLMs) possess prior knowledge of subtask planning, they may lack accurate estimations of capabilities and costs of tools to determine which to apply in each subtask. Can we combine the strengths of both LLMs and graph search to find cost-efficient tool paths? We propose a three-stage approach \"CoSTA*\" that leverages LLMs to create a subtask tree, which helps prune a graph of AI tools for the given task, and then conducts A* search on the small subgraph to find a tool path. To better balance the total cost and quality, CoSTA* combines both metrics of each tool on every subtask to guide the A* search. Each subtask's output is then evaluated by a vision-language model (VLM), where a failure will trigger an update of the tool's cost and quality on the subtask. Hence, the A* search can recover from failures quickly to explore other paths. Moreover, CoSTA* can automatically switch between modalities across subtasks for a better cost-quality trade-off. We build a novel benchmark of challenging multi-turn image editing, on which CoSTA* outperforms state-of-the-art image-editing models or agents in terms of both cost and quality, and performs versatile trade-offs upon user preference.",
        "comments": ""
      }
    ],
    [
      {
        "id": "2503.10602",
        "title": "TruthPrInt: Mitigating LVLM Object Hallucination Via Latent Truthful-Guided Pre-Intervention",
        "url": "http://arxiv.org/abs/2503.10602",
        "update_date": "2025-03-13",
        "first_author": "Jinhao Duan",
        "authors": "Jinhao Duan, Fei Kong, Hao Cheng, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Xiaofeng Zhu, Xiaoshuang Shi, Kaidi Xu",
        "category": "cs.CV",
        "abstract": "Object Hallucination (OH) has been acknowledged as one of the major trustworthy challenges in Large Vision-Language Models (LVLMs). Recent advancements in Large Language Models (LLMs) indicate that internal states, such as hidden states, encode the \"overall truthfulness\" of generated responses. However, it remains under-explored how internal states in LVLMs function and whether they could serve as \"per-token\" hallucination indicators, which is essential for mitigating OH. In this paper, we first conduct an in-depth exploration of LVLM internal states in relation to OH issues and discover that (1) LVLM internal states are high-specificity per-token indicators of hallucination behaviors. Moreover, (2) different LVLMs encode universal patterns of hallucinations in common latent subspaces, indicating that there exist \"generic truthful directions\" shared by various LVLMs. Based on these discoveries, we propose Truthful-Guided Pre-Intervention (TruthPrInt) that first learns the truthful direction of LVLM decoding and then applies truthful-guided inference-time intervention during LVLM decoding. We further propose ComnHallu to enhance both cross-LVLM and cross-data hallucination detection transferability by constructing and aligning hallucination latent subspaces. We evaluate TruthPrInt in extensive experimental settings, including in-domain and out-of-domain scenarios, over popular LVLMs and OH benchmarks. Experimental results indicate that TruthPrInt significantly outperforms state-of-the-art methods. Codes will be available at https://github.com/jinhaoduan/TruthPrInt.",
        "comments": "15 pages, 9 figures, the first two authors contributed equally"
      },
      {
        "id": "2503.10596",
        "title": "GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding",
        "url": "http://arxiv.org/abs/2503.10596",
        "update_date": "2025-03-13",
        "first_author": "Rui Hu",
        "authors": "Rui Hu, Lianghui Zhu, Yuxuan Zhang, Tianheng Cheng, Lei Liu, Heng Liu, Longjin Ran, Xiaoxin Chen, Wenyu Liu, Xinggang Wang",
        "category": "cs.CV",
        "abstract": "Pixel grounding, encompassing tasks such as Referring Expression Segmentation (RES), has garnered considerable attention due to its immense potential for bridging the gap between vision and language modalities. However, advancements in this domain are currently constrained by limitations inherent in existing datasets, including limited object categories, insufficient textual diversity, and a scarcity of high-quality annotations. To mitigate these limitations, we introduce GroundingSuite, which comprises: (1) an automated data annotation framework leveraging multiple Vision-Language Model (VLM) agents; (2) a large-scale training dataset encompassing 9.56 million diverse referring expressions and their corresponding segmentations; and (3) a meticulously curated evaluation benchmark consisting of 3,800 images. The GroundingSuite training dataset facilitates substantial performance improvements, enabling models trained on it to achieve state-of-the-art results. Specifically, a cIoU of 68.9 on gRefCOCO and a gIoU of 55.3 on RefCOCOm. Moreover, the GroundingSuite annotation framework demonstrates superior efficiency compared to the current leading data annotation method, i.e., $4.5 \\times$ faster than the GLaMM.",
        "comments": "Work in progress. Code: https://github.com/hustvl/GroundingSuite"
      },
      {
        "id": "2503.10586",
        "title": "Unlock the Power of Unlabeled Data in Language Driving Model",
        "url": "http://arxiv.org/abs/2503.10586",
        "update_date": "2025-03-13",
        "first_author": "Chaoqun Wang",
        "authors": "Chaoqun Wang, Jie Yang, Xiaobin Hong, Ruimao Zhang",
        "category": "cs.CV",
        "abstract": "Recent Vision-based Large Language Models~(VisionLLMs) for autonomous driving have seen rapid advancements. However, such promotion is extremely dependent on large-scale high-quality annotated data, which is costly and labor-intensive. To address this issue, we propose unlocking the value of abundant yet unlabeled data to improve the language-driving model in a semi-supervised learning manner. Specifically, we first introduce a series of template-based prompts to extract scene information, generating questions that create pseudo-answers for the unlabeled data based on a model trained with limited labeled data. Next, we propose a Self-Consistency Refinement method to improve the quality of these pseudo-annotations, which are later used for further training. By utilizing a pre-trained VisionLLM (e.g., InternVL), we build a strong Language Driving Model (LDM) for driving scene question-answering, outperforming previous state-of-the-art methods. Extensive experiments on the DriveLM benchmark show that our approach performs well with just 5% labeled data, achieving competitive performance against models trained with full datasets. In particular, our LDM achieves 44.85% performance with limited labeled data, increasing to 54.27% when using unlabeled data, while models trained with full datasets reach 60.68% on the DriveLM benchmark.",
        "comments": "Accepted by ICRA2025"
      },
      {
        "id": "2503.10582",
        "title": "VisualWebInstruct: Scaling up Multimodal Instruction Data through Web Search",
        "url": "http://arxiv.org/abs/2503.10582",
        "update_date": "2025-03-13",
        "first_author": "Yiming Jia",
        "authors": "Yiming Jia, Jiachen Li, Xiang Yue, Bo Li, Ping Nie, Kai Zou, Wenhu Chen",
        "category": "cs.CV",
        "abstract": "Vision-Language Models have made significant progress on many perception-focused tasks, however, their progress on reasoning-focused tasks seem to be limited due to the lack of high-quality and diverse training data. In this work, we aim to address the scarcity issue of reasoning-focused multimodal datasets. We propose VisualWebInstruct - a novel approach that leverages search engine to create a diverse, and high-quality dataset spanning multiple disciplines like math, physics, finance, chemistry, etc. Starting with meticulously selected 30,000 seed images, we employ Google Image search to identify websites containing similar images. We collect and process the HTMLs from over 700K unique URL sources. Through a pipeline of content extraction, filtering and synthesis, we build a dataset of approximately 900K question-answer pairs, with 40% being visual QA pairs and the rest as text QA pairs. Models fine-tuned on VisualWebInstruct demonstrate significant performance gains: (1) training from Llava-OV-mid shows 10-20% absolute point gains across benchmarks, (2) training from MAmmoTH-VL shows 5% absoluate gain. Our best model MAmmoTH-VL2 shows state-of-the-art performance within the 10B parameter class on MMMU-Pro-std (40.7%), MathVerse (42.6%), and DynaMath (55.7%). These remarkable results highlight the effectiveness of our dataset in enhancing VLMs' reasoning capabilities for complex multimodal tasks.",
        "comments": "Technical Report"
      },
      {
        "id": "2503.10573",
        "title": "Unveiling the Mathematical Reasoning in DeepSeek Models: A Comparative Study of Large Language Models",
        "url": "http://arxiv.org/abs/2503.10573",
        "update_date": "2025-03-13",
        "first_author": "Afrar Jahin",
        "authors": "Afrar Jahin, Arif Hassan Zidan, Yu Bao, Shizhe Liang, Tianming Liu, Wei Zhang",
        "category": "cs.LG",
        "abstract": "With the rapid evolution of Artificial Intelligence (AI), Large Language Models (LLMs) have reshaped the frontiers of various fields, spanning healthcare, public health, engineering, science, agriculture, education, arts, humanities, and mathematical reasoning. Among these advancements, DeepSeek models have emerged as noteworthy contenders, demonstrating promising capabilities that set them apart from their peers. While previous studies have conducted comparative analyses of LLMs, few have delivered a comprehensive evaluation of mathematical reasoning across a broad spectrum of LLMs. In this work, we aim to bridge this gap by conducting an in-depth comparative study, focusing on the strengths and limitations of DeepSeek models in relation to their leading counterparts. In particular, our study systematically evaluates the mathematical reasoning performance of two DeepSeek models alongside five prominent LLMs across three independent benchmark datasets. The findings reveal several key insights: 1). DeepSeek-R1 consistently achieved the highest accuracy on two of the three datasets, demonstrating strong mathematical reasoning capabilities. 2). The distilled variant of LLMs significantly underperformed compared to its peers, highlighting potential drawbacks in using distillation techniques. 3). In terms of response time, Gemini 2.0 Flash demonstrated the fastest processing speed, outperforming other models in efficiency, which is a crucial factor for real-time applications. Beyond these quantitative assessments, we delve into how architecture, training, and optimization impact LLMs' mathematical reasoning. Moreover, our study goes beyond mere performance comparison by identifying key areas for future advancements in LLM-driven mathematical reasoning. This research enhances our understanding of LLMs' mathematical reasoning and lays the groundwork for future advancements",
        "comments": ""
      },
      {
        "id": "2503.10566",
        "title": "ASIDE: Architectural Separation of Instructions and Data in Language Models",
        "url": "http://arxiv.org/abs/2503.10566",
        "update_date": "2025-03-13",
        "first_author": "Egor Zverev",
        "authors": "Egor Zverev, Evgenii Kortukov, Alexander Panfilov, Soroush Tabesh, Alexandra Volkova, Sebastian Lapuschkin, Wojciech Samek, Christoph H. Lampert",
        "category": "cs.LG",
        "abstract": "Despite their remarkable performance, large language models lack elementary safety features, and this makes them susceptible to numerous malicious attacks. In particular, previous work has identified the absence of an intrinsic separation between instructions and data as a root cause for the success of prompt injection attacks. In this work, we propose an architectural change, ASIDE, that allows the model to clearly separate between instructions and data by using separate embeddings for them. Instead of training the embeddings from scratch, we propose a method to convert an existing model to ASIDE form by using two copies of the original model's embeddings layer, and applying an orthogonal rotation to one of them. We demonstrate the effectiveness of our method by showing (1) highly increased instruction-data separation scores without a loss in model capabilities and (2) competitive results on prompt injection benchmarks, even without dedicated safety training. Additionally, we study the working mechanism behind our method through an analysis of model representations.",
        "comments": "ICLR 2025 Workshop on Building Trust in Language Models and\n  Applications"
      },
      {
        "id": "2503.10556",
        "title": "Short-term AI literacy intervention does not reduce over-reliance on incorrect ChatGPT recommendations",
        "url": "http://arxiv.org/abs/2503.10556",
        "update_date": "2025-03-13",
        "first_author": "Brett Puppart",
        "authors": "Brett Puppart, Jaan Aru",
        "category": "cs.CY",
        "abstract": "In this study, we examined whether a short-form AI literacy intervention could reduce the adoption of incorrect recommendations from large language models. High school seniors were randomly assigned to either a control or an intervention group, which received an educational text explaining ChatGPT's working mechanism, limitations, and proper use. Participants solved math puzzles with the help of ChatGPT's recommendations, which were incorrect in half of the cases. Results showed that students adopted incorrect suggestions 52.1% of the time, indicating widespread over-reliance. The educational intervention did not significantly reduce over-reliance. Instead, it led to an increase in ignoring ChatGPT's correct recommendations. We conclude that the usage of ChatGPT is associated with over-reliance and it is not trivial to increase AI literacy to counter over-reliance.",
        "comments": ""
      },
      {
        "id": "2503.10546",
        "title": "KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for Open-Vocabulary Robotic Manipulation",
        "url": "http://arxiv.org/abs/2503.10546",
        "update_date": "2025-03-13",
        "first_author": "Zixian Liu",
        "authors": "Zixian Liu, Mingtong Zhang, Yunzhu Li",
        "category": "cs.RO",
        "abstract": "With the rapid advancement of large language models (LLMs) and vision-language models (VLMs), significant progress has been made in developing open-vocabulary robotic manipulation systems. However, many existing approaches overlook the importance of object dynamics, limiting their applicability to more complex, dynamic tasks. In this work, we introduce KUDA, an open-vocabulary manipulation system that integrates dynamics learning and visual prompting through keypoints, leveraging both VLMs and learning-based neural dynamics models. Our key insight is that a keypoint-based target specification is simultaneously interpretable by VLMs and can be efficiently translated into cost functions for model-based planning. Given language instructions and visual observations, KUDA first assigns keypoints to the RGB image and queries the VLM to generate target specifications. These abstract keypoint-based representations are then converted into cost functions, which are optimized using a learned dynamics model to produce robotic trajectories. We evaluate KUDA on a range of manipulation tasks, including free-form language instructions across diverse object categories, multi-object interactions, and deformable or granular objects, demonstrating the effectiveness of our framework. The project page is available at http://kuda-dynamics.github.io.",
        "comments": "Project website: http://kuda-dynamics.github.io"
      },
      {
        "id": "2503.10544",
        "title": "DP-GPL: Differentially Private Graph Prompt Learning",
        "url": "http://arxiv.org/abs/2503.10544",
        "update_date": "2025-03-13",
        "first_author": "Jing Xu",
        "authors": "Jing Xu, Franziska Boenisch, Iyiola Emmanuel Olatunji, Adam Dziedzic",
        "category": "cs.LG",
        "abstract": "Graph Neural Networks (GNNs) have shown remarkable performance in various applications. Recently, graph prompt learning has emerged as a powerful GNN training paradigm, inspired by advances in language and vision foundation models. Here, a GNN is pre-trained on public data and then adapted to sensitive tasks using lightweight graph prompts. However, using prompts from sensitive data poses privacy risks. In this work, we are the first to investigate these practical risks in graph prompts by instantiating a membership inference attack that reveals significant privacy leakage. We also find that the standard privacy method, DP-SGD, fails to provide practical privacy-utility trade-offs in graph prompt learning, likely due to the small number of sensitive data points used to learn the prompts. As a solution, we propose DP-GPL for differentially private graph prompt learning based on the PATE framework, that generates a graph prompt with differential privacy guarantees. Our evaluation across various graph prompt learning methods, GNN architectures, and pre-training strategies demonstrates that our algorithm achieves high utility at strong privacy, effectively mitigating privacy concerns while preserving the powerful capabilities of prompted GNNs as powerful foundation models in the graph domain.",
        "comments": ""
      },
      {
        "id": "2503.10542",
        "title": "Language Models, Graph Searching, and Supervision Adulteration: When More Supervision is Less and How to Make More More",
        "url": "http://arxiv.org/abs/2503.10542",
        "update_date": "2025-03-13",
        "first_author": "Arvid Frydenlund",
        "authors": "Arvid Frydenlund",
        "category": "cs.LG",
        "abstract": "This work concerns the path-star task, a minimal example of searching over a graph. The graph, $G$, is star-shaped with $D$ arms radiating from a start node, $s$. A language model (LM) is given $G$, $s$, and a target node $t$, which ends one of the arms and is tasked with generating the arm containing $t$. The minimal nature of this task means only a single choice needs to be made: which of the $D$ arms contains $t$?   Decoder-only LMs fail to solve this elementary task above $1/D$ chance due to a learned shortcut that absorbs training supervision. We show how this pathology is caused by excess supervision and we present a series of solutions demonstrating that the task is solvable via decoder-only LMs. We find that the task's minimal nature causes its difficulty, as it prevents task decomposition. Our solutions provide insight into the pathology and its implications for LMs trained via next-token prediction.",
        "comments": "A reduced version of this work has been accepted to the Workshop on\n  Spurious Correlation and Shortcut Learning: Foundations and Solutions (SCSL)\n  at ICLR 2025. Full version under review"
      }
    ]
  ],
  "NLP": [
    [
      {
        "id": "2503.10515",
        "title": "Probing LLMs for Multilingual Discourse Generalization Through a Unified Label Set",
        "url": "http://arxiv.org/abs/2503.10515",
        "update_date": "2025-03-13",
        "first_author": "Florian Eichin",
        "authors": "Florian Eichin, Yang Janet Liu, Barbara Plank, Michael A. Hedderich",
        "category": "cs.CL",
        "abstract": "Discourse understanding is essential for many NLP tasks, yet most existing work remains constrained by framework-dependent discourse representations. This work investigates whether large language models (LLMs) capture discourse knowledge that generalizes across languages and frameworks. We address this question along two dimensions: (1) developing a unified discourse relation label set to facilitate cross-lingual and cross-framework discourse analysis, and (2) probing LLMs to assess whether they encode generalizable discourse abstractions. Using multilingual discourse relation classification as a testbed, we examine a comprehensive set of 23 LLMs of varying sizes and multilingual capabilities. Our results show that LLMs, especially those with multilingual training corpora, can generalize discourse information across languages and frameworks. Further layer-wise analyses reveal that language generalization at the discourse level is most salient in the intermediate layers. Lastly, our error analysis provides an account of challenging relation classes.",
        "comments": "18 pages, 7 figures, 3 tables, code:\n  https://github.com/mainlp/discourse_probes"
      },
      {
        "id": "2503.10470",
        "title": "Statistical Analysis of Sentence Structures through ASCII, Lexical Alignment and PCA",
        "url": "http://arxiv.org/abs/2503.10470",
        "update_date": "2025-03-13",
        "first_author": "Abhijeet Sahdev",
        "authors": "Abhijeet Sahdev",
        "category": "cs.CL",
        "abstract": "While utilizing syntactic tools such as parts-of-speech (POS) tagging has helped us understand sentence structures and their distribution across diverse corpora, it is quite complex and poses a challenge in natural language processing (NLP). This study focuses on understanding sentence structure balance - usages of nouns, verbs, determiners, etc - harmoniously without relying on such tools. It proposes a novel statistical method that uses American Standard Code for Information Interchange (ASCII) codes to represent text of 11 text corpora from various sources and their lexical category alignment after using their compressed versions through PCA, and analyzes the results through histograms and normality tests such as Shapiro-Wilk and Anderson-Darling Tests. By focusing on ASCII codes, this approach simplifies text processing, although not replacing any syntactic tools but complementing them by offering it as a resource-efficient tool for assessing text balance. The story generated by Grok shows near normality indicating balanced sentence structures in LLM outputs, whereas 4 out of the remaining 10 pass the normality tests. Further research could explore potential applications in text quality evaluation and style analysis with syntactic integration for more broader tasks.",
        "comments": ""
      },
      {
        "id": "2503.10408",
        "title": "Understanding the Logical Capabilities of Large Language Models via Out-of-Context Representation Learning",
        "url": "http://arxiv.org/abs/2503.10408",
        "update_date": "2025-03-13",
        "first_author": "Jonathan Shaki",
        "authors": "Jonathan Shaki, Emanuele La Malfa, Michael Wooldridge, Sarit Kraus",
        "category": "cs.LG",
        "abstract": "We study the capabilities of Large Language Models (LLM) on binary relations, a ubiquitous concept in math employed in most reasoning, math and logic benchmarks. This work focuses on equality, inequality, and inclusion, along with the properties they satisfy, such as ir/reflexivity, a/symmetry, transitivity, and logical complexity (e.g., number of reasoning ``hops''). We propose an alternative to in-context learning that trains only the representations of newly introduced tokens, namely out-of-context representation learning. This method mitigates linguistic biases already present in a model and, differently from in-context learning, does not rely on external information or illustrations. We argue out-of-context representation learning as a better alternative to in-context learning and fine-tuning to evaluate the capabilities of LLMs on logic tasks that are the building blocks of more complex reasoning benchmarks.",
        "comments": ""
      },
      {
        "id": "2503.10354",
        "title": "A Hybrid Architecture with Efficient Fine Tuning for Abstractive Patent Document Summarization",
        "url": "http://arxiv.org/abs/2503.10354",
        "update_date": "2025-03-13",
        "first_author": "Nevidu Jayatilleke",
        "authors": "Nevidu Jayatilleke, Ruvan Weerasinghe",
        "category": "cs.CL",
        "abstract": "Automatic patent summarization approaches that help in the patent analysis and comprehension procedure are in high demand due to the colossal growth of innovations. The development of natural language processing (NLP), text mining, and deep learning has notably amplified the efficacy of text summarization models for abundant types of documents. Summarizing patent text remains a pertinent challenge due to the labyrinthine writing style of these documents, which includes technical and legal intricacies. Additionally, these patent document contents are considerably lengthier than archetypal documents, which intricates the process of extracting pertinent information for summarization. Embodying extractive and abstractive text summarization methodologies into a hybrid framework, this study proposes a system for efficiently creating abstractive summaries of patent records. The procedure involves leveraging the LexRank graph-based algorithm to retrieve the important sentences from input parent texts, then utilizing a Bidirectional Auto-Regressive Transformer (BART) model that has been fine-tuned using Low-Ranking Adaptation (LoRA) for producing text summaries. This is accompanied by methodical testing and evaluation strategies. Furthermore, the author employed certain meta-learning techniques to achieve Domain Generalization (DG) of the abstractive component across multiple patent fields.",
        "comments": "Accepted Paper in the 8th International Research Conference on Smart\n  Computing and Systems Engineering, University of Kelaniya, Sri Lanka.\n  (Pending Publication)"
      },
      {
        "id": "2503.10310",
        "title": "Capturing Semantic Flow of ML-based Systems",
        "url": "http://arxiv.org/abs/2503.10310",
        "update_date": "2025-03-13",
        "first_author": "Shin Yoo",
        "authors": "Shin Yoo, Robert Feldt, Somin Kim, Naryeong Kim",
        "category": "cs.SE",
        "abstract": "ML-based systems are software systems that incorporates machine learning components such as Deep Neural Networks (DNNs) or Large Language Models (LLMs). While such systems enable advanced features such as high performance computer vision, natural language processing, and code generation, their internal behaviour remain largely opaque to traditional dynamic analysis such as testing: existing analysis typically concern only what is observable from the outside, such as input similarity or class label changes. We propose semantic flow, a concept designed to capture the internal behaviour of ML-based system and to provide a platform for traditional dynamic analysis techniques to be adapted to. Semantic flow combines the idea of control flow with internal states taken from executions of ML-based systems, such as activation values of a specific layer in a DNN, or embeddings of LLM responses at a specific inference step of LLM agents. The resulting representation, summarised as semantic flow graphs, can capture internal decisions that are not explicitly represented in the traditional control flow of ML-based systems. We propose the idea of semantic flow, introduce two examples using a DNN and an LLM agent, and finally sketch its properties and how it can be used to adapt existing dynamic analysis techniques for use in ML-based software systems.",
        "comments": ""
      },
      {
        "id": "2503.10251",
        "title": "Numerical Error Analysis of Large Language Models",
        "url": "http://arxiv.org/abs/2503.10251",
        "update_date": "2025-03-13",
        "first_author": "Stanislav Budzinskiy",
        "authors": "Stanislav Budzinskiy, Wenyi Fang, Longbin Zeng, Philipp Petersen",
        "category": "math.NA",
        "abstract": "Large language models based on transformer architectures have become integral to state-of-the-art natural language processing applications. However, their training remains computationally expensive and exhibits instabilities, some of which are expected to be caused by finite-precision computations. We provide a theoretical analysis of the impact of round-off errors within the forward pass of a transformer architecture which yields fundamental bounds for these effects. In addition, we conduct a series of numerical experiments which demonstrate the practical relevance of our bounds. Our results yield concrete guidelines for choosing hyperparameters that mitigate round-off errors, leading to more robust and stable inference.",
        "comments": ""
      },
      {
        "id": "2503.10183",
        "title": "Through the Magnifying Glass: Adaptive Perception Magnification for Hallucination-Free VLM Decoding",
        "url": "http://arxiv.org/abs/2503.10183",
        "update_date": "2025-03-13",
        "first_author": "Shunqi Mao",
        "authors": "Shunqi Mao, Chaoyi Zhang, Weidong Cai",
        "category": "cs.CV",
        "abstract": "Existing vision-language models (VLMs) often suffer from visual hallucination, where the generated responses contain inaccuracies that are not grounded in the visual input. Efforts to address this issue without model finetuning primarily mitigate hallucination by reducing biases contrastively or amplifying the weights of visual embedding during decoding. However, these approaches improve visual perception at the cost of impairing the language reasoning capability. In this work, we propose the Perception Magnifier (PM), a novel visual decoding method that iteratively isolates relevant visual tokens based on attention and magnifies the corresponding regions, spurring the model to concentrate on fine-grained visual details during decoding. Specifically, by magnifying critical regions while preserving the structural and contextual information at each decoding step, PM allows the VLM to enhance its scrutiny of the visual input, hence producing more accurate and faithful responses. Extensive experimental results demonstrate that PM not only achieves superior hallucination mitigation but also enhances language generation while preserving strong reasoning capabilities.Code is available at https://github.com/ShunqiM/PM .",
        "comments": "19 pages, 5 figures, 9 tables"
      },
      {
        "id": "2503.10174",
        "title": "Sensitivity-Based Distributed Programming for Non-Convex Optimization",
        "url": "http://arxiv.org/abs/2503.10174",
        "update_date": "2025-03-13",
        "first_author": "Maximilian Pierer von Esch",
        "authors": "Maximilian Pierer von Esch, Andreas Völz, Knut Graichen",
        "category": "math.OC",
        "abstract": "This paper presents a novel sensitivity-based distributed programming (SBDP) approach for non-convex, large-scale nonlinear programs (NLP). The algorithm relies on first-order sensitivities to cooperatively solve the central NLP in a distributed manner with only neighbor-to-neighbor communication and parallelizable local computations. The scheme is based on primal decomposition and offers minimal algorithmic complexity. We derive sufficient local convergence conditions for non-convex problems. Furthermore, we consider the SBDP method in a distributed optimal control context and derive favorable convergence properties in this setting. We illustrate these theoretical findings and the performance of the proposed algorithm with simulations of various distributed optimization and control problems.",
        "comments": ""
      },
      {
        "id": "2503.10150",
        "title": "Retrieval-Augmented Generation with Hierarchical Knowledge",
        "url": "http://arxiv.org/abs/2503.10150",
        "update_date": "2025-03-13",
        "first_author": "Haoyu Huang",
        "authors": "Haoyu Huang, Yongfeng Huang, Junjie Yang, Zhenyu Pan, Yongqiang Chen, Kaili Ma, Hongzhi Chen, James Cheng",
        "category": "cs.CL",
        "abstract": "Graph-based Retrieval-Augmented Generation (RAG) methods have significantly enhanced the performance of large language models (LLMs) in domain-specific tasks. However, existing RAG methods do not adequately utilize the naturally inherent hierarchical knowledge in human cognition, which limits the capabilities of RAG systems. In this paper, we introduce a new RAG approach, called HiRAG, which utilizes hierarchical knowledge to enhance the semantic understanding and structure capturing capabilities of RAG systems in the indexing and retrieval processes. Our extensive experiments demonstrate that HiRAG achieves significant performance improvements over the state-of-the-art baseline methods. The code of our proposed method is available at \\href{https://github.com/hhy-huang/HiRAG}{https://github.com/hhy-huang/HiRAG}.",
        "comments": ""
      },
      {
        "id": "2503.10094",
        "title": "Semantic Synergy: Unlocking Policy Insights and Learning Pathways Through Advanced Skill Mapping",
        "url": "http://arxiv.org/abs/2503.10094",
        "update_date": "2025-03-13",
        "first_author": "Phoebe Koundouri",
        "authors": "Phoebe Koundouri, Conrad Landis, Georgios Feretzakis",
        "category": "cs.AI",
        "abstract": "This research introduces a comprehensive system based on state-of-the-art natural language processing, semantic embedding, and efficient search techniques for retrieving similarities and thus generating actionable insights from raw textual information. The system automatically extracts and aggregates normalized competencies from multiple documents (such as policy files and curricula vitae) and creates strong relationships between recognized competencies, occupation profiles, and related learning courses. To validate its performance, we conducted a multi-tier evaluation that included both explicit and implicit skill references in synthetic and real-world documents. The results showed near-human-level accuracy, with F1 scores exceeding 0.95 for explicit skill detection and above 0.93 for implicit mentions. The system thereby establishes a sound foundation for supporting in-depth collaboration across the AE4RIA network. The methodology involves a multi-stage pipeline based on extensive preprocessing and data cleaning, semantic embedding and segmentation via SentenceTransformer, and skill extraction using a FAISS-based search method. The extracted skills are associated with occupation frameworks (as formulated in the ESCO ontology) and with learning paths offered through the Sustainable Development Goals Academy. Moreover, interactive visualization software, implemented with Dash and Plotly, presents graphs and tables for real-time exploration and informed decision-making by those involved in policymaking, training and learning supply, career transitions, and recruitment. Overall, this system, backed by rigorous validation, offers promising prospects for improved policymaking, human resource development, and lifelong learning by providing structured and actionable insights from raw, complex textual information.",
        "comments": ""
      }
    ],
    [
      {
        "id": "2503.10081",
        "title": "AdvPaint: Protecting Images from Inpainting Manipulation via Adversarial Attention Disruption",
        "url": "http://arxiv.org/abs/2503.10081",
        "update_date": "2025-03-13",
        "first_author": "Joonsung Jeon",
        "authors": "Joonsung Jeon, Woo Jae Kim, Suhyeon Ha, Sooel Son, Sung-eui Yoon",
        "category": "cs.CV",
        "abstract": "The outstanding capability of diffusion models in generating high-quality images poses significant threats when misused by adversaries. In particular, we assume malicious adversaries exploiting diffusion models for inpainting tasks, such as replacing a specific region with a celebrity. While existing methods for protecting images from manipulation in diffusion-based generative models have primarily focused on image-to-image and text-to-image tasks, the challenge of preventing unauthorized inpainting has been rarely addressed, often resulting in suboptimal protection performance. To mitigate inpainting abuses, we propose ADVPAINT, a novel defensive framework that generates adversarial perturbations that effectively disrupt the adversary's inpainting tasks. ADVPAINT targets the self- and cross-attention blocks in a target diffusion inpainting model to distract semantic understanding and prompt interactions during image generation. ADVPAINT also employs a two-stage perturbation strategy, dividing the perturbation region based on an enlarged bounding box around the object, enhancing robustness across diverse masks of varying shapes and sizes. Our experimental results demonstrate that ADVPAINT's perturbations are highly effective in disrupting the adversary's inpainting tasks, outperforming existing methods; ADVPAINT attains over a 100-point increase in FID and substantial decreases in precision.",
        "comments": "Accepted to ICLR 2025"
      },
      {
        "id": "2503.09956",
        "title": "Exploring Mutual Empowerment Between Wireless Networks and RL-based LLMs: A Survey",
        "url": "http://arxiv.org/abs/2503.09956",
        "update_date": "2025-03-13",
        "first_author": "Yu Qiao",
        "authors": "Yu Qiao, Phuong-Nam Tran, Ji Su Yoon, Loc X. Nguyen, Choong Seon Hong",
        "category": "cs.LG",
        "abstract": "Reinforcement learning (RL)-based large language models (LLMs), such as ChatGPT, DeepSeek, and Grok-3, have gained significant attention for their exceptional capabilities in natural language processing and multimodal data understanding. Meanwhile, the rapid expansion of information services has driven the growing need for intelligence, efficient, and adaptable wireless networks. Wireless networks require the empowerment of RL-based LLMs while these models also benefit from wireless networks to broaden their application scenarios. Specifically, RL-based LLMs can enhance wireless communication systems through intelligent resource allocation, adaptive network optimization, and real-time decision-making. Conversely, wireless networks provide a vital infrastructure for the efficient training, deployment, and distributed inference of RL-based LLMs, especially in decentralized and edge computing environments. This mutual empowerment highlights the need for a deeper exploration of the interplay between these two domains. We first review recent advancements in wireless communications, highlighting the associated challenges and potential solutions. We then discuss the progress of RL-based LLMs, focusing on key technologies for LLM training, challenges, and potential solutions. Subsequently, we explore the mutual empowerment between these two fields, highlighting key motivations, open challenges, and potential solutions. Finally, we provide insights into future directions, applications, and their societal impact to further explore this intersection, paving the way for next-generation intelligent communication systems. Overall, this survey provides a comprehensive overview of the relationship between RL-based LLMs and wireless networks, offering a vision where these domains empower each other to drive innovations.",
        "comments": "25 pages, 13 figures"
      },
      {
        "id": "2503.09949",
        "title": "UVE: Are MLLMs Unified Evaluators for AI-Generated Videos?",
        "url": "http://arxiv.org/abs/2503.09949",
        "update_date": "2025-03-13",
        "first_author": "Yuanxin Liu",
        "authors": "Yuanxin Liu, Rui Zhu, Shuhuai Ren, Jiacong Wang, Haoyuan Guo, Xu Sun, Lu Jiang",
        "category": "cs.CV",
        "abstract": "With the rapid growth of video generative models (VGMs), it is essential to develop reliable and comprehensive automatic metrics for AI-generated videos (AIGVs). Existing methods either use off-the-shelf models optimized for other tasks or rely on human assessment data to train specialized evaluators. These approaches are constrained to specific evaluation aspects and are difficult to scale with the increasing demands for finer-grained and more comprehensive evaluations. To address this issue, this work investigates the feasibility of using multimodal large language models (MLLMs) as a unified evaluator for AIGVs, leveraging their strong visual perception and language understanding capabilities. To evaluate the performance of automatic metrics in unified AIGV evaluation, we introduce a benchmark called UVE-Bench. UVE-Bench collects videos generated by state-of-the-art VGMs and provides pairwise human preference annotations across 15 evaluation aspects. Using UVE-Bench, we extensively evaluate 16 MLLMs. Our empirical results suggest that while advanced MLLMs (e.g., Qwen2VL-72B and InternVL2.5-78B) still lag behind human evaluators, they demonstrate promising ability in unified AIGV evaluation, significantly surpassing existing specialized evaluation methods. Additionally, we conduct an in-depth analysis of key design choices that impact the performance of MLLM-driven evaluators, offering valuable insights for future research on AIGV evaluation. The code is available at https://github.com/bytedance/UVE.",
        "comments": ""
      },
      {
        "id": "2503.09927",
        "title": "Developing and Evaluating an AI-Assisted Prediction Model for Unplanned Intensive Care Admissions following Elective Neurosurgery using Natural Language Processing within an Electronic Healthcare Record System",
        "url": "http://arxiv.org/abs/2503.09927",
        "update_date": "2025-03-13",
        "first_author": "Julia Ive",
        "authors": "Julia Ive, Olatomiwa Olukoya, Jonathan P. Funnell, James Booker, Sze H M Lam, Ugan Reddy, Kawsar Noor, Richard JB Dobson, Astri M. V. Luoma, Hani J Marcus",
        "category": "cs.CL",
        "abstract": "Introduction: Timely care in a specialised neuro-intensive therapy unit (ITU) reduces mortality and hospital stays, with planned admissions being safer than unplanned ones. However, post-operative care decisions remain subjective. This study used artificial intelligence (AI), specifically natural language processing (NLP) to analyse electronic health records (EHRs) and predict ITU admissions for elective surgery patients. Methods: This study analysed the EHRs of elective neurosurgery patients from University College London Hospital (UCLH) using NLP. Patients were categorised into planned high dependency unit (HDU) or ITU admission; unplanned HDU or ITU admission; or ward / overnight recovery (ONR). The Medical Concept Annotation Tool (MedCAT) was used to identify SNOMED-CT concepts within the clinical notes. We then explored the utility of these identified concepts for a range of AI algorithms trained to predict ITU admission. Results: The CogStack-MedCAT NLP model, initially trained on hospital-wide EHRs, underwent two refinements: first with data from patients with Normal Pressure Hydrocephalus (NPH) and then with data from Vestibular Schwannoma (VS) patients, achieving a concept detection F1-score of 0.93. This refined model was then used to extract concepts from EHR notes of 2,268 eligible neurosurgical patients. We integrated the extracted concepts into AI models, including a decision tree model and a neural time-series model. Using the simpler decision tree model, we achieved a recall of 0.87 (CI 0.82 - 0.91) for ITU admissions, reducing the proportion of unplanned ITU cases missed by human experts from 36% to 4%. Conclusion: The NLP model, refined for accuracy, has proven its efficiency in extracting relevant concepts, providing a reliable basis for predictive AI models to use in clinically valid applications.",
        "comments": ""
      },
      {
        "id": "2503.09896",
        "title": "A Rule Based Solution to Co-reference Resolution in Clinical Text",
        "url": "http://arxiv.org/abs/2503.09896",
        "update_date": "2025-03-12",
        "first_author": "Ping Chen",
        "authors": "Ping Chen, David Hinote, Guoqing Chen",
        "category": "cs.CL",
        "abstract": "Objective: The aim of this study was to build an effective co-reference resolution system tailored for the biomedical domain. Materials and Methods: Experiment materials used in this study is provided by the 2011 i2b2 Natural Language Processing Challenge. The 2011 i2b2 challenge involves coreference resolution in medical documents. Concept mentions have been annotated in clinical texts, and the mentions that co-refer in each document are to be linked by coreference chains. Normally, there are two ways of constructing a system to automatically discover co-referent links. One is to manually build rules for co-reference resolution, and the other category of approaches is to use machine learning systems to learn automatically from training datasets and then perform the resolution task on testing datasets. Results: Experiments show the existing co-reference resolution systems are able to find some of the co-referent links, and our rule based system performs well finding the majority of the co-referent links. Our system achieved 89.6% overall performance on multiple medical datasets. Conclusion: The experiment results show that manually crafted rules based on observation of training data is a valid way to accomplish high performance in this coreference resolution task for the critical biomedical domain.",
        "comments": ""
      },
      {
        "id": "2503.09894",
        "title": "What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models",
        "url": "http://arxiv.org/abs/2503.09894",
        "update_date": "2025-03-12",
        "first_author": "Abhipsha Das",
        "authors": "Abhipsha Das, Nicholas Lourie, Siavash Golkar, Mariel Pettee",
        "category": "cs.CL",
        "abstract": "The scientific literature's exponential growth makes it increasingly challenging to navigate and synthesize knowledge across disciplines. Large language models (LLMs) are powerful tools for understanding scientific text, but they fail to capture detailed relationships across large bodies of work. Unstructured approaches, like retrieval augmented generation, can sift through such corpora to recall relevant facts; however, when millions of facts influence the answer, unstructured approaches become cost prohibitive. Structured representations offer a natural complement -- enabling systematic analysis across the whole corpus. Recent work enhances LLMs with unstructured or semistructured representations of scientific concepts; to complement this, we try extracting structured representations using LLMs. By combining LLMs' semantic understanding with a schema of scientific concepts, we prototype a system that answers precise questions about the literature as a whole. Our schema applies across scientific fields and we extract concepts from it using only 20 manually annotated abstracts. To demonstrate the system, we extract concepts from 30,000 papers on arXiv spanning astrophysics, fluid dynamics, and evolutionary biology. The resulting database highlights emerging trends and, by visualizing the knowledge graph, offers new ways to explore the ever-growing landscape of scientific knowledge. Demo: abby101/surveyor-0 on HF Spaces. Code: https://github.com/chiral-carbon/kg-for-science.",
        "comments": "9 pages, 5 pdf figures"
      },
      {
        "id": "2503.09822",
        "title": "Generative AI for Named Entity Recognition in Low-Resource Language Nepali",
        "url": "http://arxiv.org/abs/2503.09822",
        "update_date": "2025-03-12",
        "first_author": "Sameer Neupane",
        "authors": "Sameer Neupane, Jeevan Chapagain, Nobal B. Niraula, Diwa Koirala",
        "category": "cs.CL",
        "abstract": "Generative Artificial Intelligence (GenAI), particularly Large Language Models (LLMs), has significantly advanced Natural Language Processing (NLP) tasks, such as Named Entity Recognition (NER), which involves identifying entities like person, location, and organization names in text. LLMs are especially promising for low-resource languages due to their ability to learn from limited data. However, the performance of GenAI models for Nepali, a low-resource language, has not been thoroughly evaluated. This paper investigates the application of state-of-the-art LLMs for Nepali NER, conducting experiments with various prompting techniques to assess their effectiveness. Our results provide insights into the challenges and opportunities of using LLMs for NER in low-resource settings and offer valuable contributions to the advancement of NLP research in languages like Nepali.",
        "comments": "This paper has been accepted in the FLAIRS Conference 2025"
      },
      {
        "id": "2503.09791",
        "title": "Minimal Time Series Transformer",
        "url": "http://arxiv.org/abs/2503.09791",
        "update_date": "2025-03-12",
        "first_author": "Joni-Kristian Kämäräinen",
        "authors": "Joni-Kristian Kämäräinen",
        "category": "cs.LG",
        "abstract": "Transformer is the state-of-the-art model for many natural language processing, computer vision, and audio analysis problems. Transformer effectively combines information from the past input and output samples in auto-regressive manner so that each sample becomes aware of all inputs and outputs. In sequence-to-sequence (Seq2Seq) modeling, the transformer processed samples become effective in predicting the next output. Time series forecasting is a Seq2Seq problem. The original architecture is defined for discrete input and output sequence tokens, but to adopt it for time series, the model must be adapted for continuous data. This work introduces minimal adaptations to make the original transformer architecture suitable for continuous value time series data.",
        "comments": "8 pages, 8 figures"
      },
      {
        "id": "2503.09790",
        "title": "Constrained Language Generation with Discrete Diffusion Models",
        "url": "http://arxiv.org/abs/2503.09790",
        "update_date": "2025-03-12",
        "first_author": "Michael Cardei",
        "authors": "Michael Cardei, Jacob K Christopher, Thomas Hartvigsen, Brian R. Bartoldson, Bhavya Kailkhura, Ferdinando Fioretto",
        "category": "cs.CL",
        "abstract": "Constraints are critical in text generation as LLM outputs are often unreliable when it comes to ensuring generated outputs adhere to user defined instruction or general safety guidelines. To address this gap, we present Constrained Discrete Diffusion (CDD), a novel method for enforcing constraints on natural language by integrating discrete diffusion models with differentiable optimization. Unlike conventional text generators, which often rely on post-hoc filtering or model retraining for controllable generation, we propose imposing constraints directly into the discrete diffusion sampling process. We illustrate how this technique can be applied to satisfy a variety of natural language constraints, including (i) toxicity mitigation by preventing harmful content from emerging, (ii) character and sequence level lexical constraints, and (iii) novel molecule sequence generation with specific property adherence. Experimental results show that our constraint-aware procedure achieves high fidelity in meeting these requirements while preserving fluency and semantic coherence, outperforming auto-regressive and existing discrete diffusion approaches.",
        "comments": ""
      },
      {
        "id": "2503.09701",
        "title": "Have LLMs Made Active Learning Obsolete? Surveying the NLP Community",
        "url": "http://arxiv.org/abs/2503.09701",
        "update_date": "2025-03-12",
        "first_author": "Julia Romberg",
        "authors": "Julia Romberg, Christopher Schröder, Julius Gonsior, Katrin Tomanek, Fredrik Olsson",
        "category": "cs.CL",
        "abstract": "Supervised learning relies on annotated data, which is expensive to obtain. A longstanding strategy to reduce annotation costs is active learning, an iterative process, in which a human annotates only data instances deemed informative by a model. Large language models (LLMs) have pushed the effectiveness of active learning, but have also improved methods such as few- or zero-shot learning, and text synthesis - thereby introducing potential alternatives. This raises the question: has active learning become obsolete? To answer this fully, we must look beyond literature to practical experiences. We conduct an online survey in the NLP community to collect previously intangible insights on the perceived relevance of data annotation, particularly focusing on active learning, including best practices, obstacles and expected future developments. Our findings show that annotated data remains a key factor, and active learning continues to be relevant. While the majority of active learning users find it effective, a comparison with a community survey from over a decade ago reveals persistent challenges: setup complexity, estimation of cost reduction, and tooling. We publish an anonymized version of the collected dataset",
        "comments": ""
      }
    ]
  ],
  "LLM Agents": [
    [
      {
        "id": "2503.10310",
        "title": "Capturing Semantic Flow of ML-based Systems",
        "url": "http://arxiv.org/abs/2503.10310",
        "update_date": "2025-03-13",
        "first_author": "Shin Yoo",
        "authors": "Shin Yoo, Robert Feldt, Somin Kim, Naryeong Kim",
        "category": "cs.SE",
        "abstract": "ML-based systems are software systems that incorporates machine learning components such as Deep Neural Networks (DNNs) or Large Language Models (LLMs). While such systems enable advanced features such as high performance computer vision, natural language processing, and code generation, their internal behaviour remain largely opaque to traditional dynamic analysis such as testing: existing analysis typically concern only what is observable from the outside, such as input similarity or class label changes. We propose semantic flow, a concept designed to capture the internal behaviour of ML-based system and to provide a platform for traditional dynamic analysis techniques to be adapted to. Semantic flow combines the idea of control flow with internal states taken from executions of ML-based systems, such as activation values of a specific layer in a DNN, or embeddings of LLM responses at a specific inference step of LLM agents. The resulting representation, summarised as semantic flow graphs, can capture internal decisions that are not explicitly represented in the traditional control flow of ML-based systems. We propose the idea of semantic flow, introduce two examples using a DNN and an LLM agent, and finally sketch its properties and how it can be used to adapt existing dynamic analysis techniques for use in ML-based software systems.",
        "comments": ""
      },
      {
        "id": "2503.10265",
        "title": "SurgRAW: Multi-Agent Workflow with Chain-of-Thought Reasoning for Surgical Intelligence",
        "url": "http://arxiv.org/abs/2503.10265",
        "update_date": "2025-03-13",
        "first_author": "Chang Han Low",
        "authors": "Chang Han Low, Ziyue Wang, Tianyi Zhang, Zhitao Zeng, Zhu Zhuo, Evangelos B. Mazomenos, Yueming Jin",
        "category": "cs.AI",
        "abstract": "Integration of Vision-Language Models (VLMs) in surgical intelligence is hindered by hallucinations, domain knowledge gaps, and limited understanding of task interdependencies within surgical scenes, undermining clinical reliability. While recent VLMs demonstrate strong general reasoning and thinking capabilities, they still lack the domain expertise and task-awareness required for precise surgical scene interpretation. Although Chain-of-Thought (CoT) can structure reasoning more effectively, current approaches rely on self-generated CoT steps, which often exacerbate inherent domain gaps and hallucinations. To overcome this, we present SurgRAW, a CoT-driven multi-agent framework that delivers transparent, interpretable insights for most tasks in robotic-assisted surgery. By employing specialized CoT prompts across five tasks: instrument recognition, action recognition, action prediction, patient data extraction, and outcome assessment, SurgRAW mitigates hallucinations through structured, domain-aware reasoning. Retrieval-Augmented Generation (RAG) is also integrated to external medical knowledge to bridge domain gaps and improve response reliability. Most importantly, a hierarchical agentic system ensures that CoT-embedded VLM agents collaborate effectively while understanding task interdependencies, with a panel discussion mechanism promotes logical consistency. To evaluate our method, we introduce SurgCoTBench, the first reasoning-based dataset with structured frame-level annotations. With comprehensive experiments, we demonstrate the effectiveness of proposed SurgRAW with 29.32% accuracy improvement over baseline VLMs on 12 robotic procedures, achieving the state-of-the-art performance and advancing explainable, trustworthy, and autonomous surgical assistance.",
        "comments": ""
      },
      {
        "id": "2503.10248",
        "title": "LLM Agents Display Human Biases but Exhibit Distinct Learning Patterns",
        "url": "http://arxiv.org/abs/2503.10248",
        "update_date": "2025-03-13",
        "first_author": "Idan Horowitz",
        "authors": "Idan Horowitz, Ori Plonsky",
        "category": "cs.AI",
        "abstract": "We investigate the choice patterns of Large Language Models (LLMs) in the context of Decisions from Experience tasks that involve repeated choice and learning from feedback, and compare their behavior to human participants. We find that on the aggregate, LLMs appear to display behavioral biases similar to humans: both exhibit underweighting rare events and correlation effects. However, more nuanced analyses of the choice patterns reveal that this happens for very different reasons. LLMs exhibit strong recency biases, unlike humans, who appear to respond in more sophisticated ways. While these different processes may lead to similar behavior on average, choice patterns contingent on recent events differ vastly between the two groups. Specifically, phenomena such as ``surprise triggers change\" and the ``wavy recency effect of rare events\" are robustly observed in humans, but entirely absent in LLMs. Our findings provide insights into the limitations of using LLMs to simulate and predict humans in learning environments and highlight the need for refined analyses of their behavior when investigating whether they replicate human decision making tendencies.",
        "comments": ""
      },
      {
        "id": "2503.10241",
        "title": "SCOOP: A Framework for Proactive Collaboration and Social Continual Learning through Natural Language Interaction andCausal Reasoning",
        "url": "http://arxiv.org/abs/2503.10241",
        "update_date": "2025-03-13",
        "first_author": "Dimitri Ognibene",
        "authors": "Dimitri Ognibene, Sabrina Patania, Luca Annese, Cansu Koyuturk, Franca Garzotto, Giuseppe Vizzari, Azzurra Ruggeri, Simone Colombani",
        "category": "cs.MA",
        "abstract": "Multimodal information-gathering settings, where users collaborate with AI in dynamic environments, are increasingly common. These involve complex processes with textual and multimodal interactions, often requiring additional structural information via cost-incurring requests. AI helpers lack access to users' true goals, beliefs, and preferences and struggle to integrate diverse information effectively.   We propose a social continual learning framework for causal knowledge acquisition and collaborative decision-making. It focuses on autonomous agents learning through dialogues, question-asking, and interaction in open, partially observable environments. A key component is a natural language oracle that answers the agent's queries about environmental mechanisms and states, refining causal understanding while balancing exploration or learning, and exploitation or knowledge use.   Evaluation tasks inspired by developmental psychology emphasize causal reasoning and question-asking skills. They complement benchmarks by assessing the agent's ability to identify knowledge gaps, generate meaningful queries, and incrementally update reasoning. The framework also evaluates how knowledge acquisition costs are amortized across tasks within the same environment.   We propose two architectures: 1) a system combining Large Language Models (LLMs) with the ReAct framework and question-generation, and 2) an advanced system with a causal world model, symbolic, graph-based, or subsymbolic, for reasoning and decision-making. The latter builds a causal knowledge graph for efficient inference and adaptability under constraints. Challenges include integrating causal reasoning into ReAct and optimizing exploration and question-asking in error-prone scenarios. Beyond applications, this framework models developmental processes combining causal reasoning, question generation, and social learning.",
        "comments": "5 pages"
      },
      {
        "id": "2503.10071",
        "title": "Advanced Tool Learning and Selection System (ATLASS): A Closed-Loop Framework Using LLM",
        "url": "http://arxiv.org/abs/2503.10071",
        "update_date": "2025-03-13",
        "first_author": "Mohd Ariful Haque",
        "authors": "Mohd Ariful Haque, Justin Williams, Sunzida Siddique, Md. Hujaifa Islam, Hasmot Ali, Kishor Datta Gupta, Roy George",
        "category": "cs.AI",
        "abstract": "The combination of LLM agents with external tools enables models to solve complex tasks beyond their knowledge base. Human-designed tools are inflexible and restricted to solutions within the scope of pre-existing tools created by experts. To address this problem, we propose ATLASS, an advanced tool learning and selection system designed as a closed-loop framework. It enables the LLM to solve problems by dynamically generating external tools on demand. In this framework, agents play a crucial role in orchestrating tool selection, execution, and refinement, ensuring adaptive problem-solving capabilities. The operation of ATLASS follows three phases: The first phase, Understanding Tool Requirements, involves the Agents determining whether tools are required and specifying their functionality; the second phase, Tool Retrieval/Generation, involves the Agents retrieving or generating tools based on their availability; and the third phase, Task Solving, involves combining all the component tools necessary to complete the initial task. The Tool Dataset stores the generated tools, ensuring reusability and minimizing inference cost. Current LLM-based tool generation systems have difficulty creating complex tools that need APIs or external packages. In ATLASS, we solve the problem by automatically setting up the environment, fetching relevant API documentation online, and using a Python interpreter to create a reliable, versatile tool that works in a wider range of situations. OpenAI GPT-4.0 is used as the LLM agent, and safety and ethical concerns are handled through human feedback before executing generated code. By addressing the limitations of predefined toolsets and enhancing adaptability, ATLASS serves as a real-world solution that empowers users with dynamically generated tools for complex problem-solving.",
        "comments": ""
      },
      {
        "id": "2503.10009",
        "title": "OR-LLM-Agent: Automating Modeling and Solving of Operations Research Optimization Problem with Reasoning Large Language Model",
        "url": "http://arxiv.org/abs/2503.10009",
        "update_date": "2025-03-13",
        "first_author": "Bowen Zhang",
        "authors": "Bowen Zhang, Pengcheng Luo",
        "category": "cs.AI",
        "abstract": "Operations Research (OR) has been widely applied in various fields such as resource allocation, production planning, and supply chain management. However, addressing real-world OR problems requires OR experts to perform mathematical modeling and programmers to develop solution algorithms. This traditional method, heavily reliant on experts, is costly and has long development cycles, severely limiting the widespread adoption of OR techniques. Few have considered using Artificial Intelligence (AI) to replace professionals to achieve fully automated solutions for OR problems. We propose OR-LLM-Agent, the first AI agent that enables end-to-end automation for solving real-world OR problems. OR-LLM-Agent leverages the Chain-of-Thought (CoT) reasoning capabilities of Large Language Models (LLMs) to translate natural language problem descriptions into formal mathematical models and automatically generate Gurobi solver code. In OR-LLM-Agent, OR-CodeAgent is designed to automate code execution and repair within a sandbox environment, facilitating the derivation of the final solution. Due to the lack of dedicated benchmark datasets for evaluating the automated solving of OR problems, we construct a benchmark dataset comprising 83 real-world OR problems described in natural language. We conduct comparative experiments with state-of-the-art (SOTA) reasoning LLMs, including GPT-o3-mini, DeepSeek-R1, and Gemini 2.0 Flash Thinking. The OR-LLM-Agent achieved the highest pass rate of 100% and the highest solution accuracy of 85%, demonstrating the feasibility of automated OR problem-solving. Data and code have been publicly available at https://github.com/bwz96sco/or_llm_agent.",
        "comments": "11 pages, 6 figures"
      },
      {
        "id": "2503.09385",
        "title": "PCLA: A Framework for Testing Autonomous Agents in the CARLA Simulator",
        "url": "http://arxiv.org/abs/2503.09385",
        "update_date": "2025-03-13",
        "first_author": "Masoud Jamshidiyan Tehrani",
        "authors": "Masoud Jamshidiyan Tehrani, Jinhan Kim, Paolo Tonella",
        "category": "cs.SE",
        "abstract": "Recent research on testing autonomous driving agents has grown significantly, especially in simulation environments. The CARLA simulator is often the preferred choice, and the autonomous agents from the CARLA Leaderboard challenge are regarded as the best-performing agents within this environment. However, researchers who test these agents, rather than training their own ones from scratch, often face challenges in utilizing them within customized test environments and scenarios. To address these challenges, we introduce PCLA (Pretrained CARLA Leaderboard Agents), an open-source Python testing framework that includes nine high-performing pre-trained autonomous agents from the Leaderboard challenges. PCLA is the first infrastructure specifically designed for testing various autonomous agents in arbitrary CARLA environments/scenarios. PCLA provides a simple way to deploy Leaderboard agents onto a vehicle without relying on the Leaderboard codebase, it allows researchers to easily switch between agents without requiring modifications to CARLA versions or programming environments, and it is fully compatible with the latest version of CARLA while remaining independent of the Leaderboard's specific CARLA version. PCLA is publicly accessible at https://github.com/MasoudJTehrani/PCLA.",
        "comments": "This work will be published at the FSE 2025 demonstration track"
      },
      {
        "id": "2503.09794",
        "title": "Augmenting Teamwork through AI Agents as Spatial Collaborators",
        "url": "http://arxiv.org/abs/2503.09794",
        "update_date": "2025-03-12",
        "first_author": "Mariana Fernandez-Espinosa",
        "authors": "Mariana Fernandez-Espinosa, Diego Gomez-Zara",
        "category": "cs.HC",
        "abstract": "As Augmented Reality (AR) and Artificial Intelligence (AI) continue to converge, new opportunities emerge for AI agents to actively support human collaboration in immersive environments. While prior research has primarily focused on dyadic human-AI interactions, less attention has been given to Human-AI Teams (HATs) in AR, where AI acts as an adaptive teammate rather than a static tool. This position paper takes the perspective of team dynamics and work organization to propose that AI agents in AR should not only interact with individuals but also recognize and respond to team-level needs in real time. We argue that spatially aware AI agents should dynamically generate the resources necessary for effective collaboration, such as virtual blackboards for brainstorming, mental map models for shared understanding, and memory recall of spatial configurations to enhance knowledge retention and task coordination. This approach moves beyond predefined AI assistance toward context-driven AI interventions that optimize team performance and decision-making.",
        "comments": "Positional paper for the CHI 2025 Everyday AR through AI-in-the-Loop\n  Workshop"
      },
      {
        "id": "2503.09780",
        "title": "AgentDAM: Privacy Leakage Evaluation for Autonomous Web Agents",
        "url": "http://arxiv.org/abs/2503.09780",
        "update_date": "2025-03-12",
        "first_author": "Arman Zharmagambetov",
        "authors": "Arman Zharmagambetov, Chuan Guo, Ivan Evtimov, Maya Pavlova, Ruslan Salakhutdinov, Kamalika Chaudhuri",
        "category": "cs.AI",
        "abstract": "LLM-powered AI agents are an emerging frontier with tremendous potential to increase human productivity. However, empowering AI agents to take action on their user's behalf in day-to-day tasks involves giving them access to potentially sensitive and private information, which leads to a possible risk of inadvertent privacy leakage when the agent malfunctions. In this work, we propose one way to address that potential risk, by training AI agents to better satisfy the privacy principle of data minimization. For the purposes of this benchmark, by \"data minimization\" we mean instances where private information is shared only when it is necessary to fulfill a specific task-relevant purpose. We develop a benchmark called AgentDAM to evaluate how well existing and future AI agents can limit processing of potentially private information that we designate \"necessary\" to fulfill the task. Our benchmark simulates realistic web interaction scenarios and is adaptable to all existing web navigation agents. We use AgentDAM to evaluate how well AI agents built on top of GPT-4, Llama-3 and Claude can limit processing of potentially private information when unnecessary, and show that these agents are often prone to inadvertent use of unnecessary sensitive information. We finally propose a prompting-based approach that reduces this.",
        "comments": "project page: https://github.com/facebookresearch/ai-agent-privacy"
      },
      {
        "id": "2503.09447",
        "title": "Online Language Splatting",
        "url": "http://arxiv.org/abs/2503.09447",
        "update_date": "2025-03-12",
        "first_author": "Saimouli Katragadda",
        "authors": "Saimouli Katragadda, Cho-Ying Wu, Yuliang Guo, Xinyu Huang, Guoquan Huang, Liu Ren",
        "category": "cs.AI",
        "abstract": "To enable AI agents to interact seamlessly with both humans and 3D environments, they must not only perceive the 3D world accurately but also align human language with 3D spatial representations. While prior work has made significant progress by integrating language features into geometrically detailed 3D scene representations using 3D Gaussian Splatting (GS), these approaches rely on computationally intensive offline preprocessing of language features for each input image, limiting adaptability to new environments. In this work, we introduce Online Language Splatting, the first framework to achieve online, near real-time, open-vocabulary language mapping within a 3DGS-SLAM system without requiring pre-generated language features. The key challenge lies in efficiently fusing high-dimensional language features into 3D representations while balancing the computation speed, memory usage, rendering quality and open-vocabulary capability. To this end, we innovatively design: (1) a high-resolution CLIP embedding module capable of generating detailed language feature maps in 18ms per frame, (2) a two-stage online auto-encoder that compresses 768-dimensional CLIP features to 15 dimensions while preserving open-vocabulary capabilities, and (3) a color-language disentangled optimization approach to improve rendering quality. Experimental results show that our online method not only surpasses the state-of-the-art offline methods in accuracy but also achieves more than 40x efficiency boost, demonstrating the potential for dynamic and interactive AI applications.",
        "comments": ""
      }
    ],
    [
      {
        "id": "2503.09263",
        "title": "COLA: A Scalable Multi-Agent Framework For Windows UI Task Automation",
        "url": "http://arxiv.org/abs/2503.09263",
        "update_date": "2025-03-12",
        "first_author": "Di Zhao",
        "authors": "Di Zhao, Longhui Ma, Siwei Wang, Miao Wang, Zhao Lv",
        "category": "cs.MA",
        "abstract": "With the rapid advancements in Large Language Models (LLMs), an increasing number of studies have leveraged LLMs as the cognitive core of agents to address complex task decision-making challenges. Specially, recent research has demonstrated the potential of LLM-based agents on automating Windows GUI operations. However, existing methodologies exhibit two critical challenges: (1) static agent architectures fail to dynamically adapt to the heterogeneous requirements of OS-level tasks, leading to inadequate scenario generalization;(2) the agent workflows lack fault tolerance mechanism, necessitating complete process re-execution for UI agent decision error. To address these limitations, we introduce \\textit{COLA}, a collaborative multi-agent framework for automating Windows UI operations. In this framework, a scenario-aware agent Task Scheduler decomposes task requirements into atomic capability units, dynamically selects the optimal agent from a decision agent pool, effectively responds to the capability requirements of diverse scenarios. The decision agent pool supports plug-and-play expansion for enhanced flexibility. In addition, we design a memory unit equipped to all agents for their self-evolution. Furthermore, we develop an interactive backtracking mechanism that enables human to intervene to trigger state rollbacks for non-destructive process repair. Our experimental results on the GAIA benchmark demonstrates that the \\textit{COLA} framework achieves state-of-the-art performance with an average score of 31.89\\%, significantly outperforming baseline approaches without web API integration. Ablation studies further validate the individual contributions of our dynamic scheduling. The code is available at https://github.com/Alokia/COLA-demo.",
        "comments": ""
      },
      {
        "id": "2503.09648",
        "title": "A Survey on Trustworthy LLM Agents: Threats and Countermeasures",
        "url": "http://arxiv.org/abs/2503.09648",
        "update_date": "2025-03-12",
        "first_author": "Miao Yu",
        "authors": "Miao Yu, Fanci Meng, Xinyun Zhou, Shilong Wang, Junyuan Mao, Linsey Pang, Tianlong Chen, Kun Wang, Xinfeng Li, Yongfeng Zhang, Bo An, Qingsong Wen",
        "category": "cs.MA",
        "abstract": "With the rapid evolution of Large Language Models (LLMs), LLM-based agents and Multi-agent Systems (MAS) have significantly expanded the capabilities of LLM ecosystems. This evolution stems from empowering LLMs with additional modules such as memory, tools, environment, and even other agents. However, this advancement has also introduced more complex issues of trustworthiness, which previous research focused solely on LLMs could not cover. In this survey, we propose the TrustAgent framework, a comprehensive study on the trustworthiness of agents, characterized by modular taxonomy, multi-dimensional connotations, and technical implementation. By thoroughly investigating and summarizing newly emerged attacks, defenses, and evaluation methods for agents and MAS, we extend the concept of Trustworthy LLM to the emerging paradigm of Trustworthy Agent. In TrustAgent, we begin by deconstructing and introducing various components of the Agent and MAS. Then, we categorize their trustworthiness into intrinsic (brain, memory, and tool) and extrinsic (user, agent, and environment) aspects. Subsequently, we delineate the multifaceted meanings of trustworthiness and elaborate on the implementation techniques of existing research related to these internal and external modules. Finally, we present our insights and outlook on this domain, aiming to provide guidance for future endeavors.",
        "comments": ""
      },
      {
        "id": "2503.09102",
        "title": "\"I Like Your Story!\": A Co-Creative Story-Crafting Game with a Persona-Driven Character Based on Generative AI",
        "url": "http://arxiv.org/abs/2503.09102",
        "update_date": "2025-03-12",
        "first_author": "Jiaying Fu",
        "authors": "Jiaying Fu, Xiruo Wang, Zhouyi Li, Kate Vi, Chuyan Xu, Yuqian Sun",
        "category": "cs.HC",
        "abstract": "While generative AI is advancing writing support tools, creative writing is often seen as the exclusive domain of skilled writers. This paper introduces \"1001 Nights\", a co-creative story-crafting game that transforms writing into a playful and rewarding activity. In this game, the AI agent takes on the role of a \"moody\" king with distinct storytelling preferences, not merely assisting but actively influencing the narrative. Players engage with the king agent through strategic storytelling, guiding him to mention weapon-related keywords, which materialize as battle equipment. The king agent provides dynamic feedback, expressing satisfaction or displeasure, prompting players to adjust their approach. By combining storytelling, game mechanics, and AI-driven responses, our system motivates creativity through playful constraints. Inspired by Oulipo's literary techniques, this approach demonstrates how AI-powered game experiences can make creative writing more accessible and engaging, encouraging players to explore their creative potential.",
        "comments": "5 pages, 5 figures, In Extended Abstracts of the CHI Conference on\n  Human Factors in Computing Systems (CHI EA '25)"
      },
      {
        "id": "2503.09089",
        "title": "LocAgent: Graph-Guided LLM Agents for Code Localization",
        "url": "http://arxiv.org/abs/2503.09089",
        "update_date": "2025-03-12",
        "first_author": "Zhaoling Chen",
        "authors": "Zhaoling Chen, Xiangru Tang, Gangda Deng, Fang Wu, Jialong Wu, Zhiwei Jiang, Viktor Prasanna, Arman Cohan, Xingyao Wang",
        "category": "cs.SE",
        "abstract": "Code localization--identifying precisely where in a codebase changes need to be made--is a fundamental yet challenging task in software maintenance. Existing approaches struggle to efficiently navigate complex codebases when identifying relevant code sections. The challenge lies in bridging natural language problem descriptions with the appropriate code elements, often requiring reasoning across hierarchical structures and multiple dependencies. We introduce LocAgent, a framework that addresses code localization through graph-based representation. By parsing codebases into directed heterogeneous graphs, LocAgent creates a lightweight representation that captures code structures (files, classes, functions) and their dependencies (imports, invocations, inheritance), enabling LLM agents to effectively search and locate relevant entities through powerful multi-hop reasoning. Experimental results on real-world benchmarks demonstrate that our approach significantly enhances accuracy in code localization. Notably, our method with the fine-tuned Qwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA proprietary models at greatly reduced cost (approximately 86% reduction), reaching up to 92.7% accuracy on file-level localization while improving downstream GitHub issue resolution success rates by 12% for multiple attempts (Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent.",
        "comments": ""
      },
      {
        "id": "2503.08979",
        "title": "Agentic AI for Scientific Discovery: A Survey of Progress, Challenges, and Future Directions",
        "url": "http://arxiv.org/abs/2503.08979",
        "update_date": "2025-03-12",
        "first_author": "Mourad Gridach",
        "authors": "Mourad Gridach, Jay Nanavati, Khaldoun Zine El Abidine, Lenon Mendes, Christina Mack",
        "category": "cs.CL",
        "abstract": "The integration of Agentic AI into scientific discovery marks a new frontier in research automation. These AI systems, capable of reasoning, planning, and autonomous decision-making, are transforming how scientists perform literature review, generate hypotheses, conduct experiments, and analyze results. This survey provides a comprehensive overview of Agentic AI for scientific discovery, categorizing existing systems and tools, and highlighting recent progress across fields such as chemistry, biology, and materials science. We discuss key evaluation metrics, implementation frameworks, and commonly used datasets to offer a detailed understanding of the current state of the field. Finally, we address critical challenges, such as literature review automation, system reliability, and ethical concerns, while outlining future research directions that emphasize human-AI collaboration and enhanced system calibration.",
        "comments": ""
      },
      {
        "id": "2503.08931",
        "title": "ARCHED: A Human-Centered Framework for Transparent, Responsible, and Collaborative AI-Assisted Instructional Design",
        "url": "http://arxiv.org/abs/2503.08931",
        "update_date": "2025-03-11",
        "first_author": "Hongming Li",
        "authors": "Hongming Li, Yizirui Fang, Shan Zhang, Seiyon M. Lee, Yiming Wang, Mark Trexler, Anthony F. Botelho",
        "category": "cs.CY",
        "abstract": "Integrating Large Language Models (LLMs) in educational technology presents unprecedented opportunities to improve instructional design (ID), yet existing approaches often prioritize automation over pedagogical rigor and human agency. This paper introduces ARCHED (AI for Responsible, Collaborative, Human-centered Education Instructional Design), a structured multi-stage framework that ensures human educators remain central in the design process while leveraging AI capabilities. Unlike traditional AI-generated instructional materials that lack transparency, ARCHED employs a cascaded workflow aligned with Bloom's taxonomy. The framework integrates specialized AI agents - one generating diverse pedagogical options and another evaluating alignment with learning objectives - while maintaining educators as primary decision-makers. This approach addresses key limitations in current AI-assisted instructional design, ensuring transparency, pedagogical foundation, and meaningful human agency. Empirical evaluations demonstrate that ARCHED enhances instructional design quality while preserving educator oversight, marking a step forward in responsible AI integration in education.",
        "comments": "Accepted to the iRAISE Workshop at AAAI 2025. To be published in PMLR\n  Volume 273"
      },
      {
        "id": "2503.08506",
        "title": "ReviewAgents: Bridging the Gap Between Human and AI-Generated Paper Reviews",
        "url": "http://arxiv.org/abs/2503.08506",
        "update_date": "2025-03-11",
        "first_author": "Xian Gao",
        "authors": "Xian Gao, Jiacheng Ruan, Jingsheng Gao, Ting Liu, Yuzhuo Fu",
        "category": "cs.CL",
        "abstract": "Academic paper review is a critical yet time-consuming task within the research community. With the increasing volume of academic publications, automating the review process has become a significant challenge. The primary issue lies in generating comprehensive, accurate, and reasoning-consistent review comments that align with human reviewers' judgments. In this paper, we address this challenge by proposing ReviewAgents, a framework that leverages large language models (LLMs) to generate academic paper reviews. We first introduce a novel dataset, Review-CoT, consisting of 142k review comments, designed for training LLM agents. This dataset emulates the structured reasoning process of human reviewers-summarizing the paper, referencing relevant works, identifying strengths and weaknesses, and generating a review conclusion. Building upon this, we train LLM reviewer agents capable of structured reasoning using a relevant-paper-aware training method. Furthermore, we construct ReviewAgents, a multi-role, multi-LLM agent review framework, to enhance the review comment generation process. Additionally, we propose ReviewBench, a benchmark for evaluating the review comments generated by LLMs. Our experimental results on ReviewBench demonstrate that while existing LLMs exhibit a certain degree of potential for automating the review process, there remains a gap when compared to human-generated reviews. Moreover, our ReviewAgents framework further narrows this gap, outperforming advanced LLMs in generating review comments.",
        "comments": "Work in progress"
      },
      {
        "id": "2503.08336",
        "title": "Talk2PC: Enhancing 3D Visual Grounding through LiDAR and Radar Point Clouds Fusion for Autonomous Driving",
        "url": "http://arxiv.org/abs/2503.08336",
        "update_date": "2025-03-11",
        "first_author": "Runwei Guan",
        "authors": "Runwei Guan, Jianan Liu, Ningwei Ouyang, Daizong Liu, Xiaolou Sun, Lianqing Zheng, Ming Xu, Yutao Yue, Hui Xiong",
        "category": "cs.CV",
        "abstract": "Embodied outdoor scene understanding forms the foundation for autonomous agents to perceive, analyze, and react to dynamic driving environments. However, existing 3D understanding is predominantly based on 2D Vision-Language Models (VLMs), collecting and processing limited scene-aware contexts. Instead, compared to the 2D planar visual information, point cloud sensors like LiDAR offer rich depth information and fine-grained 3D representations of objects. Meanwhile, the emerging 4D millimeter-wave (mmWave) radar is capable of detecting the motion trend, velocity, and reflection intensity of each object. Therefore, the integration of these two modalities provides more flexible querying conditions for natural language, enabling more accurate 3D visual grounding. To this end, in this paper, we exploratively propose a novel method called TPCNet, the first outdoor 3D visual grounding model upon the paradigm of prompt-guided point cloud sensor combination, including both LiDAR and radar contexts. To adaptively balance the features of these two sensors required by the prompt, we have designed a multi-fusion paradigm called Two-Stage Heterogeneous Modal Adaptive Fusion. Specifically, this paradigm initially employs Bidirectional Agent Cross-Attention (BACA), which feeds dual-sensor features, characterized by global receptive fields, to the text features for querying. Additionally, we have designed a Dynamic Gated Graph Fusion (DGGF) module to locate the regions of interest identified by the queries. To further enhance accuracy, we innovatively devise an C3D-RECHead, based on the nearest object edge. Our experiments have demonstrated that our TPCNet, along with its individual modules, achieves the state-of-the-art performance on both the Talk2Radar and Talk2Car datasets.",
        "comments": "14 pages, 11 figures"
      },
      {
        "id": "2503.08308",
        "title": "Seeing and Reasoning with Confidence: Supercharging Multimodal LLMs with an Uncertainty-Aware Agentic Framework",
        "url": "http://arxiv.org/abs/2503.08308",
        "update_date": "2025-03-11",
        "first_author": "Zhuo Zhi",
        "authors": "Zhuo Zhi, Chen Feng, Adam Daneshmend, Mine Orlu, Andreas Demosthenous, Lu Yin, Da Li, Ziquan Liu, Miguel R. D. Rodrigues",
        "category": "cs.AI",
        "abstract": "Multimodal large language models (MLLMs) show promise in tasks like visual question answering (VQA) but still face challenges in multimodal reasoning. Recent works adapt agentic frameworks or chain-of-thought (CoT) reasoning to improve performance. However, CoT-based multimodal reasoning often demands costly data annotation and fine-tuning, while agentic approaches relying on external tools risk introducing unreliable output from these tools. In this paper, we propose Seeing and Reasoning with Confidence (SRICE), a training-free multimodal reasoning framework that integrates external vision models with uncertainty quantification (UQ) into an MLLM to address these challenges. Specifically, SRICE guides the inference process by allowing MLLM to autonomously select regions of interest through multi-stage interactions with the help of external tools. We propose to use a conformal prediction-based approach to calibrate the output of external tools and select the optimal tool by estimating the uncertainty of an MLLM's output. Our experiment shows that the average improvement of SRICE over the base MLLM is 4.6% on five datasets and the performance on some datasets even outperforms fine-tuning-based methods, revealing the significance of ensuring reliable tool use in an MLLM agent.",
        "comments": ""
      },
      {
        "id": "2503.08275",
        "title": "Beyond Outlining: Heterogeneous Recursive Planning for Adaptive Long-form Writing with Language Models",
        "url": "http://arxiv.org/abs/2503.08275",
        "update_date": "2025-03-11",
        "first_author": "Ruibin Xiong",
        "authors": "Ruibin Xiong, Yimeng Chen, Dmitrii Khizbullin, Jürgen Schmidhuber",
        "category": "cs.AI",
        "abstract": "Long-form writing agents require flexible integration and interaction across information retrieval, reasoning, and composition. Current approaches rely on predetermined workflows and rigid thinking patterns to generate outlines before writing, resulting in constrained adaptability during writing. In this paper we propose a general agent framework that achieves human-like adaptive writing through recursive task decomposition and dynamic integration of three fundamental task types, i.e. retrieval, reasoning, and composition. Our methodology features: 1) a planning mechanism that interleaves recursive task decomposition and execution, eliminating artificial restrictions on writing workflow; and 2) integration of task types that facilitates heterogeneous task decomposition. Evaluations on both fiction writing and technical report generation show that our method consistently outperforms state-of-the-art approaches across all automatic evaluation metrics, which demonstrate the effectiveness and broad applicability of our proposed framework.",
        "comments": "29 pages, 2 figures"
      }
    ]
  ],
  "Reinforcement Learning with LLMs": [
    [
      {
        "id": "2503.10626",
        "title": "NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models",
        "url": "http://arxiv.org/abs/2503.10626",
        "update_date": "2025-03-13",
        "first_author": "Mert Albaba",
        "authors": "Mert Albaba, Chenhao Li, Markos Diomataris, Omid Taheri, Andreas Krause, Michael Black",
        "category": "cs.CV",
        "abstract": "Acquiring physically plausible motor skills across diverse and unconventional morphologies-including humanoid robots, quadrupeds, and animals-is essential for advancing character simulation and robotics. Traditional methods, such as reinforcement learning (RL) are task- and body-specific, require extensive reward function engineering, and do not generalize well. Imitation learning offers an alternative but relies heavily on high-quality expert demonstrations, which are difficult to obtain for non-human morphologies. Video diffusion models, on the other hand, are capable of generating realistic videos of various morphologies, from humans to ants. Leveraging this capability, we propose a data-independent approach for skill acquisition that learns 3D motor skills from 2D-generated videos, with generalization capability to unconventional and non-human forms. Specifically, we guide the imitation learning process by leveraging vision transformers for video-based comparisons by calculating pair-wise distance between video embeddings. Along with video-encoding distance, we also use a computed similarity between segmented video frames as a guidance reward. We validate our method on locomotion tasks involving unique body configurations. In humanoid robot locomotion tasks, we demonstrate that 'No-data Imitation Learning' (NIL) outperforms baselines trained on 3D motion-capture data. Our results highlight the potential of leveraging generative video models for physically plausible skill learning with diverse morphologies, effectively replacing data collection with data generation for imitation learning.",
        "comments": ""
      },
      {
        "id": "2503.10615",
        "title": "R1-Onevision: Advancing Generalized Multimodal Reasoning through Cross-Modal Formalization",
        "url": "http://arxiv.org/abs/2503.10615",
        "update_date": "2025-03-13",
        "first_author": "Yi Yang",
        "authors": "Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, Bo Zhang, Wei Chen",
        "category": "cs.CV",
        "abstract": "Large Language Models have demonstrated remarkable reasoning capability in complex textual tasks. However, multimodal reasoning, which requires integrating visual and textual information, remains a significant challenge. Existing visual-language models often struggle to effectively analyze and reason visual content, resulting in suboptimal performance on complex reasoning tasks. Moreover, the absence of comprehensive benchmarks hinders the accurate assessment of multimodal reasoning capabilities. In this paper, we introduce R1-Onevision, a multimodal reasoning model designed to bridge the gap between visual perception and deep reasoning. To achieve this, we propose a cross-modal reasoning pipeline that transforms images into formal textural representations, enabling precise language-based reasoning. Leveraging this pipeline, we construct the R1-Onevision dataset which provides detailed, step-by-step multimodal reasoning annotations across diverse domains. We further develop the R1-Onevision model through supervised fine-tuning and reinforcement learning to cultivate advanced reasoning and robust generalization abilities. To comprehensively evaluate multimodal reasoning performance across different grades, we introduce R1-Onevision-Bench, a benchmark aligned with human educational stages, covering exams from junior high school to university and beyond. Experimental results show that R1-Onevision achieves state-of-the-art performance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple challenging multimodal reasoning benchmarks.",
        "comments": "Code and Model: https://github.com/Fancy-MLLM/R1-onevision"
      },
      {
        "id": "2503.10561",
        "title": "The Lagrangian Method for Solving Constrained Markov Games",
        "url": "http://arxiv.org/abs/2503.10561",
        "update_date": "2025-03-13",
        "first_author": "Soham Das",
        "authors": "Soham Das, Santiago Paternain, Luiz F. O. Chamon, Ceyhun Eksin",
        "category": "math.OC",
        "abstract": "We propose the concept of a Lagrangian game to solve constrained Markov games. Such games model scenarios where agents face cost constraints in addition to their individual rewards, that depend on both agent joint actions and the evolving environment state over time. Constrained Markov games form the formal mechanism behind safe multiagent reinforcement learning, providing a structured model for dynamic multiagent interactions in a multitude of settings, such as autonomous teams operating under local energy and time constraints, for example. We develop a primal-dual approach in which agents solve a Lagrangian game associated with the current Lagrange multiplier, simulate cost and reward trajectories over a fixed horizon, and update the multiplier using accrued experience. This update rule generates a new Lagrangian game, initiating the next iteration. Our key result consists in showing that the sequence of solutions to these Lagrangian games yields a nonstationary Nash solution for the original constrained Markov game.",
        "comments": "19 pages, 6 figures"
      },
      {
        "id": "2503.10559",
        "title": "Towards Safe Path Tracking Using the Simplex Architecture",
        "url": "http://arxiv.org/abs/2503.10559",
        "update_date": "2025-03-13",
        "first_author": "Georg Jäger",
        "authors": "Georg Jäger, Nils-Jonathan Friedrich, Hauke Petersen, Benjamin Noack",
        "category": "cs.RO",
        "abstract": "Robot navigation in complex environments necessitates controllers that are adaptive and safe. Traditional controllers like Regulated Pure Pursuit, Dynamic Window Approach, and Model-Predictive Path Integral, while reliable, struggle to adapt to dynamic conditions. Reinforcement Learning offers adaptability but lacks formal safety guarantees. To address this, we propose a path tracking controller leveraging the Simplex architecture. It combines a Reinforcement Learning controller for adaptiveness and performance with a high-assurance controller providing safety and stability. Our contribution is twofold. We firstly discuss general stability and safety considerations for designing controllers using the Simplex architecture. Secondly, we present a Simplex-based path tracking controller. Our simulation results, supported by preliminary in-field tests, demonstrate the controller's effectiveness in maintaining safety while achieving comparable performance to state-of-the-art methods.",
        "comments": ""
      },
      {
        "id": "2503.10509",
        "title": "SySLLM: Generating Synthesized Policy Summaries for Reinforcement Learning Agents Using Large Language Models",
        "url": "http://arxiv.org/abs/2503.10509",
        "update_date": "2025-03-13",
        "first_author": "Sahar Admoni",
        "authors": "Sahar Admoni, Omer Ben-Porat, Ofra Amir",
        "category": "cs.LG",
        "abstract": "Policies generated by Reinforcement Learning (RL) algorithms can be difficult to describe to users, as they result from the interplay between complex reward structures and neural network-based representations. This combination often leads to unpredictable behaviors, making policies challenging to analyze and posing significant obstacles to fostering human trust in real-world applications. Global policy summarization methods aim to describe agent behavior through a demonstration of actions in a subset of world-states. However, users can only watch a limited number of demonstrations, restricting their understanding of policies. Moreover, those methods overly rely on user interpretation, as they do not synthesize observations into coherent patterns. In this work, we present SySLLM (Synthesized Summary using LLMs), a novel method that employs synthesis summarization, utilizing large language models' (LLMs) extensive world knowledge and ability to capture patterns, to generate textual summaries of policies. Specifically, an expert evaluation demonstrates that the proposed approach generates summaries that capture the main insights generated by experts while not resulting in significant hallucinations. Additionally, a user study shows that SySLLM summaries are preferred over demonstration-based policy summaries and match or surpass their performance in objective agent identification tasks.",
        "comments": ""
      },
      {
        "id": "2503.10484",
        "title": "Learning Robotic Policy with Imagined Transition: Mitigating the Trade-off between Robustness and Optimality",
        "url": "http://arxiv.org/abs/2503.10484",
        "update_date": "2025-03-13",
        "first_author": "Wei Xiao",
        "authors": "Wei Xiao, Shangke Lyu, Zhefei Gong, Renjie Wang, Donglin Wang",
        "category": "cs.RO",
        "abstract": "Existing quadrupedal locomotion learning paradigms usually rely on extensive domain randomization to alleviate the sim2real gap and enhance robustness. It trains policies with a wide range of environment parameters and sensor noises to perform reliably under uncertainty. However, since optimal performance under ideal conditions often conflicts with the need to handle worst-case scenarios, there is a trade-off between optimality and robustness. This trade-off forces the learned policy to prioritize stability in diverse and challenging conditions over efficiency and accuracy in ideal ones, leading to overly conservative behaviors that sacrifice peak performance. In this paper, we propose a two-stage framework that mitigates this trade-off by integrating policy learning with imagined transitions. This framework enhances the conventional reinforcement learning (RL) approach by incorporating imagined transitions as demonstrative inputs. These imagined transitions are derived from an optimal policy and a dynamics model operating within an idealized setting. Our findings indicate that this approach significantly mitigates the domain randomization-induced negative impact of existing RL algorithms. It leads to accelerated training, reduced tracking errors within the distribution, and enhanced robustness outside the distribution.",
        "comments": ""
      },
      {
        "id": "2503.10466",
        "title": "SortingEnv: An Extendable RL-Environment for an Industrial Sorting Process",
        "url": "http://arxiv.org/abs/2503.10466",
        "update_date": "2025-03-13",
        "first_author": "Tom Maus",
        "authors": "Tom Maus, Nico Zengeler, Tobias Glasmachers",
        "category": "cs.LG",
        "abstract": "We present a novel reinforcement learning (RL) environment designed to both optimize industrial sorting systems and study agent behavior in evolving spaces. In simulating material flow within a sorting process our environment follows the idea of a digital twin, with operational parameters like belt speed and occupancy level. To reflect real-world challenges, we integrate common upgrades to industrial setups, like new sensors or advanced machinery. It thus includes two variants: a basic version focusing on discrete belt speed adjustments and an advanced version introducing multiple sorting modes and enhanced material composition observations. We detail the observation spaces, state update mechanisms, and reward functions for both environments. We further evaluate the efficiency of common RL algorithms like Proximal Policy Optimization (PPO), Deep-Q-Networks (DQN), and Advantage Actor Critic (A2C) in comparison to a classical rule-based agent (RBA). This framework not only aids in optimizing industrial processes but also provides a foundation for studying agent behavior and transferability in evolving environments, offering insights into model performance and practical implications for real-world RL applications.",
        "comments": "Presented at the 12th International Conference on Industrial\n  Engineering and Applications (ICIEA-EU), Munich, 2025. This article has been\n  submitted to AIP Conference Proceedings. After it is published, it will be\n  available in the AIP Digital Library"
      },
      {
        "id": "2503.10460",
        "title": "Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and Beyond",
        "url": "http://arxiv.org/abs/2503.10460",
        "update_date": "2025-03-13",
        "first_author": "Liang Wen",
        "authors": "Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, Xiangzheng Zhang",
        "category": "cs.CL",
        "abstract": "This paper presents our work on the Light-R1 series, with models, data, and code all released.   We first focus on training long COT models from scratch, specifically starting from models initially lacking long COT capabilities. Using a curriculum training recipe consisting of two-stage SFT and semi-on-policy DPO, we train our model Light-R1-32B from Qwen2.5-32B-Instruct, resulting in superior math performance compared to DeepSeek-R1-Distill-Qwen-32B. Despite being trained exclusively on math data, Light-R1-32B shows strong generalization across other domains. In the subsequent phase of this work, we highlight the significant benefit of the 3k dataset constructed for the second SFT stage on enhancing other models. By fine-tuning DeepSeek-R1-Distilled models using this dataset, we obtain new SOTA models in 7B and 14B, while the 32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.   Furthermore, we extend our work by applying reinforcement learning, specifically GRPO, on long-COT models to further improve reasoning performance. We successfully train our final Light-R1-14B-DS with RL, achieving SOTA performance among 14B parameter models in math. With AIME24 & 25 scores of 74.0 and 60.2 respectively, Light-R1-14B-DS surpasses even many 32B models and DeepSeek-R1-Distill-Llama-70B. Its RL training also exhibits well expected behavior, showing simultaneous increase in response length and reward score.   The Light-R1 series of work validates training long-COT models from scratch, showcases the art in SFT data and releases SOTA models from RL.",
        "comments": "all release at https://github.com/Qihoo360/Light-R1"
      },
      {
        "id": "2503.10434",
        "title": "Finetuning Generative Trajectory Model with Reinforcement Learning from Human Feedback",
        "url": "http://arxiv.org/abs/2503.10434",
        "update_date": "2025-03-13",
        "first_author": "Derun Li",
        "authors": "Derun Li, Jianwei Ren, Yue Wang, Xin Wen, Pengxiang Li, Leimeng Xu, Kun Zhan, Zhongpu Xia, Peng Jia, Xianpeng Lang, Ningyi Xu, Hang Zhao",
        "category": "cs.RO",
        "abstract": "Generating human-like and adaptive trajectories is essential for autonomous driving in dynamic environments. While generative models have shown promise in synthesizing feasible trajectories, they often fail to capture the nuanced variability of human driving styles due to dataset biases and distributional shifts. To address this, we introduce TrajHF, a human feedback-driven finetuning framework for generative trajectory models, designed to align motion planning with diverse driving preferences. TrajHF incorporates multi-conditional denoiser and reinforcement learning with human feedback to refine multi-modal trajectory generation beyond conventional imitation learning. This enables better alignment with human driving preferences while maintaining safety and feasibility constraints. TrajHF achieves PDMS of 93.95 on NavSim benchmark, significantly exceeding other methods. TrajHF sets a new paradigm for personalized and adaptable trajectory generation in autonomous driving.",
        "comments": "10 pages, 5 figures"
      },
      {
        "id": "2503.10421",
        "title": "Towards Constraint-Based Adaptive Hypergraph Learning for Solving Vehicle Routing: An End-to-End Solution",
        "url": "http://arxiv.org/abs/2503.10421",
        "update_date": "2025-03-13",
        "first_author": "Zhenwei Wang",
        "authors": "Zhenwei Wang, Ruibin Bai, Tiehua Zhang",
        "category": "cs.LG",
        "abstract": "The application of learning based methods to vehicle routing problems has emerged as a pivotal area of research in combinatorial optimization. These problems are characterized by vast solution spaces and intricate constraints, making traditional approaches such as exact mathematical models or heuristic methods prone to high computational overhead or reliant on the design of complex heuristic operators to achieve optimal or near optimal solutions. Meanwhile, although some recent learning-based methods can produce good performance for VRP with straightforward constraint scenarios, they often fail to effectively handle hard constraints that are common in practice. This study introduces a novel end-to-end framework that combines constraint-oriented hypergraphs with reinforcement learning to address vehicle routing problems. A central innovation of this work is the development of a constraint-oriented dynamic hyperedge reconstruction strategy within an encoder, which significantly enhances hypergraph representation learning. Additionally, the decoder leverages a double-pointer attention mechanism to iteratively generate solutions. The proposed model is trained by incorporating asynchronous parameter updates informed by hypergraph constraints and optimizing a dual loss function comprising constraint loss and policy gradient loss. The experiment results on benchmark datasets demonstrate that the proposed approach not only eliminates the need for sophisticated heuristic operators but also achieves substantial improvements in solution quality.",
        "comments": ""
      }
    ],
    [
      {
        "id": "2503.10419",
        "title": "A nonlinear real time capable motion cueing algorithm based on deep reinforcement learning",
        "url": "http://arxiv.org/abs/2503.10419",
        "update_date": "2025-03-13",
        "first_author": "Hendrik Scheidel",
        "authors": "Hendrik Scheidel, Camilo Gonzalez, Houshyar Asadi, Tobias Bellmann, Andreas Seefried, Shady Mohamed, Saeid Nahavandi",
        "category": "eess.SY",
        "abstract": "In motion simulation, motion cueing algorithms are used for the trajectory planning of the motion simulator platform, where workspace limitations prevent direct reproduction of reference trajectories. Strategies such as motion washout, which return the platform to its center, are crucial in these settings. For serial robotic MSPs with highly nonlinear workspaces, it is essential to maximize the efficient utilization of the MSPs kinematic and dynamic capabilities. Traditional approaches, including classical washout filtering and linear model predictive control, fail to consider platform-specific, nonlinear properties, while nonlinear model predictive control, though comprehensive, imposes high computational demands that hinder real-time, pilot-in-the-loop application without further simplification. To overcome these limitations, we introduce a novel approach using deep reinforcement learning for motion cueing, demonstrated here for the first time in a 6-degree-of-freedom setting with full consideration of the MSPs kinematic nonlinearities. Previous work by the authors successfully demonstrated the application of DRL to a simplified 2-DOF setup, which did not consider kinematic or dynamic constraints. This approach has been extended to all 6 DOF by incorporating a complete kinematic model of the MSP into the algorithm, a crucial step for enabling its application on a real motion simulator. The training of the DRL-MCA is based on Proximal Policy Optimization in an actor-critic implementation combined with an automated hyperparameter optimization. After detailing the necessary training framework and the algorithm itself, we provide a comprehensive validation, demonstrating that the DRL MCA achieves competitive performance against established algorithms. Moreover, it generates feasible trajectories by respecting all system constraints and meets all real-time requirements with low...",
        "comments": ""
      },
      {
        "id": "2503.10395",
        "title": "Optical stabilization for laser communication satellite systems through proportional-integral-derivative (PID) control and reinforcement learning approach",
        "url": "http://arxiv.org/abs/2503.10395",
        "update_date": "2025-03-13",
        "first_author": "A. Reutov",
        "authors": "A. Reutov, S. Vorobey, A. Katanskiy, V. Balakirev, R. Bakhshaliev, K. Barbyshev, V. Merzlinkin, V. Tekaev",
        "category": "physics.ins-det",
        "abstract": "One of the main issues of the satellite-to-ground optical communication, including free-space satellite quantum key distribution (QKD), is an achievement of the reasonable accuracy of positioning, navigation and optical stabilization. Proportional-integral-derivative (PID) controllers can handle with various control tasks in optical systems. Recent research shows the promising results in the area of composite control systems including classical control via PID controllers and reinforcement learning (RL) approach. In this work we apply RL agent to an experimental stand of the optical stabilization system of QKD terminal. We find via agent control history more precise PID parameters and also provide effective combined RL-PID dynamic control approach for the optical stabilization of satellite-to-ground communication system.",
        "comments": ""
      },
      {
        "id": "2503.10357",
        "title": "Do I look like a `cat.n.01` to you? A Taxonomy Image Generation Benchmark",
        "url": "http://arxiv.org/abs/2503.10357",
        "update_date": "2025-03-13",
        "first_author": "Viktor Moskvoretskii",
        "authors": "Viktor Moskvoretskii, Alina Lobanova, Ekaterina Neminova, Chris Biemann, Alexander Panchenko, Irina Nikishina",
        "category": "cs.CL",
        "abstract": "This paper explores the feasibility of using text-to-image models in a zero-shot setup to generate images for taxonomy concepts. While text-based methods for taxonomy enrichment are well-established, the potential of the visual dimension remains unexplored. To address this, we propose a comprehensive benchmark for Taxonomy Image Generation that assesses models' abilities to understand taxonomy concepts and generate relevant, high-quality images. The benchmark includes common-sense and randomly sampled WordNet concepts, alongside the LLM generated predictions. The 12 models are evaluated using 9 novel taxonomy-related text-to-image metrics and human feedback. Moreover, we pioneer the use of pairwise evaluation with GPT-4 feedback for image generation. Experimental results show that the ranking of models differs significantly from standard T2I tasks. Playground-v2 and FLUX consistently outperform across metrics and subsets and the retrieval-based approach performs poorly. These findings highlight the potential for automating the curation of structured data resources.",
        "comments": "Labeled data and generated image Wordnet are published at\n  https://huggingface.co/collections/VityaVitalich/generated-image-wordnet-67d2c868ff1414ec2f8e0d3d"
      },
      {
        "id": "2503.10352",
        "title": "Safe exploration in reproducing kernel Hilbert spaces",
        "url": "http://arxiv.org/abs/2503.10352",
        "update_date": "2025-03-13",
        "first_author": "Abdullah Tokmak",
        "authors": "Abdullah Tokmak, Kiran G. Krishnan, Thomas B. Schön, Dominik Baumann",
        "category": "cs.LG",
        "abstract": "Popular safe Bayesian optimization (BO) algorithms learn control policies for safety-critical systems in unknown environments. However, most algorithms make a smoothness assumption, which is encoded by a known bounded norm in a reproducing kernel Hilbert space (RKHS). The RKHS is a potentially infinite-dimensional space, and it remains unclear how to reliably obtain the RKHS norm of an unknown function. In this work, we propose a safe BO algorithm capable of estimating the RKHS norm from data. We provide statistical guarantees on the RKHS norm estimation, integrate the estimated RKHS norm into existing confidence intervals and show that we retain theoretical guarantees, and prove safety of the resulting safe BO algorithm. We apply our algorithm to safely optimize reinforcement learning policies on physics simulators and on a real inverted pendulum, demonstrating improved performance, safety, and scalability compared to the state-of-the-art.",
        "comments": "Accepted to AISTATS 2025"
      },
      {
        "id": "2503.10318",
        "title": "Enhance Exploration in Safe Reinforcement Learning with Contrastive Representation Learning",
        "url": "http://arxiv.org/abs/2503.10318",
        "update_date": "2025-03-13",
        "first_author": "Duc Kien Doan",
        "authors": "Duc Kien Doan, Bang Giang Le, Viet Cuong Ta",
        "category": "cs.LG",
        "abstract": "In safe reinforcement learning, agent needs to balance between exploration actions and safety constraints. Following this paradigm, domain transfer approaches learn a prior Q-function from the related environments to prevent unsafe actions. However, because of the large number of false positives, some safe actions are never executed, leading to inadequate exploration in sparse-reward environments. In this work, we aim to learn an efficient state representation to balance the exploration and safety-prefer action in a sparse-reward environment. Firstly, the image input is mapped to latent representation by an auto-encoder. A further contrastive learning objective is employed to distinguish safe and unsafe states. In the learning phase, the latent distance is used to construct an additional safety check, which allows the agent to bias the exploration if it visits an unsafe state. To verify the effectiveness of our method, the experiment is carried out in three navigation-based MiniGrid environments. The result highlights that our method can explore the environment better while maintaining a good balance between safety and efficiency.",
        "comments": "Accepted at ACIIDS 2025"
      },
      {
        "id": "2503.10304",
        "title": "Nash Equilibrium Constrained Auto-bidding With Bi-level Reinforcement Learning",
        "url": "http://arxiv.org/abs/2503.10304",
        "update_date": "2025-03-13",
        "first_author": "Zhiyu Mou",
        "authors": "Zhiyu Mou, Miao Xu, Rongquan Bai, Zhuoran Yang, Chuan Yu, Jian Xu, Bo Zheng",
        "category": "cs.LG",
        "abstract": "Many online advertising platforms provide advertisers with auto-bidding services to enhance their advertising performance. However, most existing auto-bidding algorithms fail to accurately capture the auto-bidding problem formulation that the platform truly faces, let alone solve it. Actually, we argue that the platform should try to help optimize each advertiser's performance to the greatest extent -- which makes $\\epsilon$-Nash Equilibrium ($\\epsilon$-NE) a necessary solution concept -- while maximizing the social welfare of all the advertisers for the platform's long-term value. Based on this, we introduce the \\emph{Nash-Equilibrium Constrained Bidding} (NCB), a new formulation of the auto-bidding problem from the platform's perspective. Specifically, it aims to maximize the social welfare of all advertisers under the $\\epsilon$-NE constraint. However, the NCB problem presents significant challenges due to its constrained bi-level structure and the typically large number of advertisers involved. To address these challenges, we propose a \\emph{Bi-level Policy Gradient} (BPG) framework with theoretical guarantees. Notably, its computational complexity is independent of the number of advertisers, and the associated gradients are straightforward to compute. Extensive simulated and real-world experiments validate the effectiveness of the BPG framework.",
        "comments": ""
      },
      {
        "id": "2503.10228",
        "title": "Policy Teaching via Data Poisoning in Learning from Human Preferences",
        "url": "http://arxiv.org/abs/2503.10228",
        "update_date": "2025-03-13",
        "first_author": "Andi Nika",
        "authors": "Andi Nika, Jonathan Nöther, Debmalya Mandal, Parameswaran Kamalaruban, Adish Singla, Goran Radanović",
        "category": "cs.LG",
        "abstract": "We study data poisoning attacks in learning from human preferences. More specifically, we consider the problem of teaching/enforcing a target policy $\\pi^\\dagger$ by synthesizing preference data. We seek to understand the susceptibility of different preference-based learning paradigms to poisoned preference data by analyzing the number of samples required by the attacker to enforce $\\pi^\\dagger$. We first propose a general data poisoning formulation in learning from human preferences and then study it for two popular paradigms, namely: (a) reinforcement learning from human feedback (RLHF) that operates by learning a reward model using preferences; (b) direct preference optimization (DPO) that directly optimizes policy using preferences. We conduct a theoretical analysis of the effectiveness of data poisoning in a setting where the attacker is allowed to augment a pre-existing dataset and also study its special case where the attacker can synthesize the entire preference dataset from scratch. As our main results, we provide lower/upper bounds on the number of samples required to enforce $\\pi^\\dagger$. Finally, we discuss the implications of our results in terms of the susceptibility of these learning paradigms under such data poisoning attacks.",
        "comments": "In AISTATS 2025"
      },
      {
        "id": "2503.10215",
        "title": "Adaptive Preference Aggregation",
        "url": "http://arxiv.org/abs/2503.10215",
        "update_date": "2025-03-13",
        "first_author": "Benjamin Heymann",
        "authors": "Benjamin Heymann",
        "category": "cs.AI",
        "abstract": "AI alignment, the challenge of ensuring AI systems act in accordance with human values, has emerged as a critical problem in the development of systems such as foundation models and recommender systems. Still, the current dominant approach, reinforcement learning with human feedback (RLHF) faces known theoretical limitations in aggregating diverse human preferences. Social choice theory provides a framework to aggregate preferences, but was not developed for the multidimensional applications typical of AI. Leveraging insights from a recently published urn process, this work introduces a preference aggregation strategy that adapts to the user's context and that inherits the good properties of the maximal lottery, a Condorcet-consistent solution concept.",
        "comments": ""
      },
      {
        "id": "2503.10177",
        "title": "PRISM: Preference Refinement via Implicit Scene Modeling for 3D Vision-Language Preference-Based Reinforcement Learning",
        "url": "http://arxiv.org/abs/2503.10177",
        "update_date": "2025-03-13",
        "first_author": "Yirong Sun",
        "authors": "Yirong Sun, Yanjun Chen",
        "category": "cs.CL",
        "abstract": "We propose PRISM, a novel framework designed to overcome the limitations of 2D-based Preference-Based Reinforcement Learning (PBRL) by unifying 3D point cloud modeling and future-aware preference refinement. At its core, PRISM adopts a 3D Point Cloud-Language Model (3D-PC-LLM) to mitigate occlusion and viewpoint biases, ensuring more stable and spatially consistent preference signals. Additionally, PRISM leverages Chain-of-Thought (CoT) reasoning to incorporate long-horizon considerations, thereby preventing the short-sighted feedback often seen in static preference comparisons. In contrast to conventional PBRL techniques, this integration of 3D perception and future-oriented reasoning leads to significant gains in preference agreement rates, faster policy convergence, and robust generalization across unseen robotic environments. Our empirical results, spanning tasks such as robotic manipulation and autonomous navigation, highlight PRISM's potential for real-world applications where precise spatial understanding and reliable long-term decision-making are critical. By bridging 3D geometric awareness with CoT-driven preference modeling, PRISM establishes a comprehensive foundation for scalable, human-aligned reinforcement learning.",
        "comments": ""
      },
      {
        "id": "2503.10118",
        "title": "An Real-Sim-Real (RSR) Loop Framework for Generalizable Robotic Policy Transfer with Differentiable Simulation",
        "url": "http://arxiv.org/abs/2503.10118",
        "update_date": "2025-03-13",
        "first_author": "Lu Shi",
        "authors": "Lu Shi, Yuxuan Xu, Shiyu Wang, Jinhao Huang, Wenhao Zhao, Yufei Jia, Zike Yan, Weibin Gu, Guyue Zhou",
        "category": "cs.RO",
        "abstract": "The sim-to-real gap remains a critical challenge in robotics, hindering the deployment of algorithms trained in simulation to real-world systems. This paper introduces a novel Real-Sim-Real (RSR) loop framework leveraging differentiable simulation to address this gap by iteratively refining simulation parameters, aligning them with real-world conditions, and enabling robust and efficient policy transfer. A key contribution of our work is the design of an informative cost function that encourages the collection of diverse and representative real-world data, minimizing bias and maximizing the utility of each data point for simulation refinement. This cost function integrates seamlessly into existing reinforcement learning algorithms (e.g., PPO, SAC) and ensures a balanced exploration of critical regions in the real domain. Furthermore, our approach is implemented on the versatile Mujoco MJX platform, and our framework is compatible with a wide range of robotic systems. Experimental results on several robotic manipulation tasks demonstrate that our method significantly reduces the sim-to-real gap, achieving high task performance and generalizability across diverse scenarios of both explicit and implicit environmental uncertainties.",
        "comments": ""
      }
    ]
  ],
  "Mixture of Experts": [
    [
      {
        "id": "2503.10576",
        "title": "Sample and Map from a Single Convex Potential: Generation using Conjugate Moment Measures",
        "url": "http://arxiv.org/abs/2503.10576",
        "update_date": "2025-03-13",
        "first_author": "Nina Vesseron",
        "authors": "Nina Vesseron, Louis Béthune, Marco Cuturi",
        "category": "stat.ML",
        "abstract": "A common approach to generative modeling is to split model-fitting into two blocks: define first how to sample noise (e.g. Gaussian) and choose next what to do with it (e.g. using a single map or flows). We explore in this work an alternative route that ties sampling and mapping. We find inspiration in moment measures, a result that states that for any measure $\\rho$ supported on a compact convex set of $\\mathbb{R}^d$, there exists a unique convex potential $u$ such that $\\rho=\\nabla u\\,\\sharp\\,e^{-u}$. While this does seem to tie effectively sampling (from log-concave distribution $e^{-u}$) and action (pushing particles through $\\nabla u$), we observe on simple examples (e.g., Gaussians or 1D distributions) that this choice is ill-suited for practical tasks. We study an alternative factorization, where $\\rho$ is factorized as $\\nabla w^*\\,\\sharp\\,e^{-w}$, where $w^*$ is the convex conjugate of $w$. We call this approach conjugate moment measures, and show far more intuitive results on these examples. Because $\\nabla w^*$ is the Monge map between the log-concave distribution $e^{-w}$ and $\\rho$, we rely on optimal transport solvers to propose an algorithm to recover $w$ from samples of $\\rho$, and parameterize $w$ as an input-convex neural network.",
        "comments": ""
      },
      {
        "id": "2503.10421",
        "title": "Towards Constraint-Based Adaptive Hypergraph Learning for Solving Vehicle Routing: An End-to-End Solution",
        "url": "http://arxiv.org/abs/2503.10421",
        "update_date": "2025-03-13",
        "first_author": "Zhenwei Wang",
        "authors": "Zhenwei Wang, Ruibin Bai, Tiehua Zhang",
        "category": "cs.LG",
        "abstract": "The application of learning based methods to vehicle routing problems has emerged as a pivotal area of research in combinatorial optimization. These problems are characterized by vast solution spaces and intricate constraints, making traditional approaches such as exact mathematical models or heuristic methods prone to high computational overhead or reliant on the design of complex heuristic operators to achieve optimal or near optimal solutions. Meanwhile, although some recent learning-based methods can produce good performance for VRP with straightforward constraint scenarios, they often fail to effectively handle hard constraints that are common in practice. This study introduces a novel end-to-end framework that combines constraint-oriented hypergraphs with reinforcement learning to address vehicle routing problems. A central innovation of this work is the development of a constraint-oriented dynamic hyperedge reconstruction strategy within an encoder, which significantly enhances hypergraph representation learning. Additionally, the decoder leverages a double-pointer attention mechanism to iteratively generate solutions. The proposed model is trained by incorporating asynchronous parameter updates informed by hypergraph constraints and optimizing a dual loss function comprising constraint loss and policy gradient loss. The experiment results on benchmark datasets demonstrate that the proposed approach not only eliminates the need for sophisticated heuristic operators but also achieves substantial improvements in solution quality.",
        "comments": ""
      },
      {
        "id": "2503.10412",
        "title": "dFLMoE: Decentralized Federated Learning via Mixture of Experts for Medical Data Analysis",
        "url": "http://arxiv.org/abs/2503.10412",
        "update_date": "2025-03-13",
        "first_author": "Luyuan Xie",
        "authors": "Luyuan Xie, Tianyu Luan, Wenyuan Cai, Guochen Yan, Zhaoyu Chen, Nan Xi, Yuejian Fang, Qingni Shen, Zhonghai Wu, Junsong Yuan",
        "category": "cs.LG",
        "abstract": "Federated learning has wide applications in the medical field. It enables knowledge sharing among different healthcare institutes while protecting patients' privacy. However, existing federated learning systems are typically centralized, requiring clients to upload client-specific knowledge to a central server for aggregation. This centralized approach would integrate the knowledge from each client into a centralized server, and the knowledge would be already undermined during the centralized integration before it reaches back to each client. Besides, the centralized approach also creates a dependency on the central server, which may affect training stability if the server malfunctions or connections are unstable. To address these issues, we propose a decentralized federated learning framework named dFLMoE. In our framework, clients directly exchange lightweight head models with each other. After exchanging, each client treats both local and received head models as individual experts, and utilizes a client-specific Mixture of Experts (MoE) approach to make collective decisions. This design not only reduces the knowledge damage with client-specific aggregations but also removes the dependency on the central server to enhance the robustness of the framework. We validate our framework on multiple medical tasks, demonstrating that our method evidently outperforms state-of-the-art approaches under both model homogeneity and heterogeneity settings.",
        "comments": ""
      },
      {
        "id": "2503.10363",
        "title": "Plasmon-Mediated Hybridization of Wannier-Mott and Frenkel Excitons in a Monolayer WS2 -- J-Aggregate Hybrid System",
        "url": "http://arxiv.org/abs/2503.10363",
        "update_date": "2025-03-13",
        "first_author": "Nicolas Zorn Morales",
        "authors": "Nicolas Zorn Morales, Daniel Steffen Rühl, Sergey Sadofev, Emil List-Kratochvil, Sylke Blumstengel",
        "category": "cond-mat.mtrl-sci",
        "abstract": "We present a tunable plasmonic platform that allows room temperature hybridization of dissimilar excitons, namely of Wannier-Mott excitons in monolayer (1L) WS2 and Frenkel excitons in molecular J-aggregates via simultaneous strong coupling to surface plasmon polaritons. It is based on a simple layered design consisting of a thin planar silver film and a dielectric spacer on which monolayer and the aggregates are assembled. Strong coupling is revealed by angle-dependent spectroscopic ellipsometry measurements in total internal reflection geometry by the observation of double Rabi splitting at the two excitonic resonances. We analyze the exciton-exciton-plasmon system with the coupled oscillator model and demonstrate modulation of polariton character and dynamics by the number of molecules participating in the coupling. Furthermore, we propose a route to remotely control the mode splitting at the Frenkel excitonic resonance via electrostatic gating of the 1L-WS2 and to switch the molecule-plasmon interaction between the weak and strong coupling regime.",
        "comments": "15 pages, 4 Figures, supporting information"
      },
      {
        "id": "2503.10325",
        "title": "Collaborative Speculative Inference for Efficient LLM Inference Serving",
        "url": "http://arxiv.org/abs/2503.10325",
        "update_date": "2025-03-13",
        "first_author": "Luyao Gao",
        "authors": "Luyao Gao, Jianchun Liu, Hongli Xu, Liusheng Huang",
        "category": "cs.DC",
        "abstract": "Speculative inference is a promising paradigm employing small speculative models (SSMs) as drafters to generate draft tokens, which are subsequently verified in parallel by the target large language model (LLM). This approach enhances the efficiency of inference serving by reducing LLM inference latency and costs while preserving generation quality. However, existing speculative methods face critical challenges, including inefficient resource utilization and limited draft acceptance, which constrain their scalability and overall effectiveness. To overcome these obstacles, we present CoSine, a novel speculative inference system that decouples sequential speculative decoding from parallel verification, enabling efficient collaboration among multiple nodes. Specifically, CoSine routes inference requests to specialized drafters based on their expertise and incorporates a confidence-based token fusion mechanism to synthesize outputs from cooperating drafters, ensuring high-quality draft generation. Additionally, CoSine dynamically orchestrates the execution of speculative decoding and verification in a pipelined manner, employing batch scheduling to selectively group requests and adaptive speculation control to minimize idle periods. By optimizing parallel workflows through heterogeneous node collaboration, CoSine balances draft generation and verification throughput in real-time, thereby maximizing resource utilization. Experimental results demonstrate that CoSine achieves superior performance compared to state-of-the-art speculative approaches. Notably, with equivalent resource costs, CoSine achieves up to a 23.2% decrease in latency and a 32.5% increase in throughput compared to baseline methods.",
        "comments": ""
      },
      {
        "id": "2503.10276",
        "title": "Quantum switches for single-photon routing and entanglement generation in waveguide-based networks",
        "url": "http://arxiv.org/abs/2503.10276",
        "update_date": "2025-03-13",
        "first_author": "Juan Cumbrado",
        "authors": "Juan Cumbrado, Ricardo Puebla",
        "category": "quant-ph",
        "abstract": "The interconnection of quantum nodes holds great promise for scaling up quantum computing units and enabling information processing across long-distance quantum registers. Such quantum networks can be realized using superconducting qubits linked by waveguides, which facilitate fast and robust on-demand quantum information exchange via traveling single photons. In this article, we propose leveraging additional qubit degrees of freedom as quantum switches that coherently condition the system dynamics. These switches are implemented using a qubit dispersively coupled to transfer resonators, which mediate interactions between node qubits and quantum links. Through wavepacket shaping techniques, we demonstrate that when the switch is closed, full excitation transfer occurs as a propagating photon, whereas an open switch allows only partial transfer without distorting the shape of the emitted photon. Based on this switch mechanism, we present deterministic protocols for generating entangled states via single-photon routing across the network, such as Bell, Greenberger-Horne-Zeilinger and W states. The feasibility of our approach is validated through numerical simulations of a three-node network, incorporating decoherence and photon loss effects. Our results indicate that high-fidelity entangled states can be realized employing the proposed quantum switches in current state-of-the-art platforms.",
        "comments": "11 pages, 3 figs; comments and feedback welcome!"
      },
      {
        "id": "2503.10111",
        "title": "StableFusion: Continual Video Retrieval via Frame Adaptation",
        "url": "http://arxiv.org/abs/2503.10111",
        "update_date": "2025-03-13",
        "first_author": "Zecheng Zhao",
        "authors": "Zecheng Zhao, Zhi Chen, Zi Huang, Shazia Sadiq, Tong Chen",
        "category": "cs.CV",
        "abstract": "Text-to-Video Retrieval (TVR) aims to match videos with corresponding textual queries, yet the continual influx of new video content poses a significant challenge for maintaining system performance over time. In this work, we introduce the first benchmark for Continual Text-to-Video Retrieval (CTVR) to overcome these limitations. Our analysis reveals that current TVR methods based on pre-trained models struggle to retain plasticity when adapting to new tasks, while existing continual learning approaches experience catastrophic forgetting, resulting in semantic misalignment between historical queries and stored video features. To address these challenges, we propose StableFusion, a novel CTVR framework comprising two main components: the Frame Fusion Adapter (FFA), which captures temporal dynamics in video content while preserving model flexibility, and the Task-Aware Mixture-of-Experts (TAME), which maintains consistent semantic alignment between queries across tasks and the stored video features. Comprehensive evaluations on two benchmark datasets under various task settings demonstrate that StableFusion outperforms existing continual learning and TVR methods, achieving superior retrieval performance with minimal degradation on earlier tasks in the context of continuous video streams. Our code is available at: https://github.com/JasonCodeMaker/CTVR",
        "comments": ""
      },
      {
        "id": "2503.10088",
        "title": "Enhanced Route Planning with Calibrated Uncertainty Set",
        "url": "http://arxiv.org/abs/2503.10088",
        "update_date": "2025-03-13",
        "first_author": "Lingxuan Tang",
        "authors": "Lingxuan Tang, Rui Luo, Zhixin Zhou, Nicolo Colombo",
        "category": "cs.LG",
        "abstract": "This paper investigates the application of probabilistic prediction methodologies in route planning within a road network context. Specifically, we introduce the Conformalized Quantile Regression for Graph Autoencoders (CQR-GAE), which leverages the conformal prediction technique to offer a coverage guarantee, thus improving the reliability and robustness of our predictions. By incorporating uncertainty sets derived from CQR-GAE, we substantially improve the decision-making process in route planning under a robust optimization framework. We demonstrate the effectiveness of our approach by applying the CQR-GAE model to a real-world traffic scenario. The results indicate that our model significantly outperforms baseline methods, offering a promising avenue for advancing intelligent transportation systems.",
        "comments": "arXiv admin note: text overlap with arXiv:2406.08281"
      },
      {
        "id": "2503.10004",
        "title": "Combining Cooperative Re-Routing with Intersection Coordination for Connected and Automated Vehicles in Urban Networks",
        "url": "http://arxiv.org/abs/2503.10004",
        "update_date": "2025-03-13",
        "first_author": "Panagiotis Typaldos",
        "authors": "Panagiotis Typaldos, Andreas A. Malikopoulos",
        "category": "eess.SY",
        "abstract": "In this paper, we present a hierarchical framework that integrates upper-level routing with low-level optimal trajectory planning for connected and automated vehicles (CAVs) traveling in an urban network. The upper-level controller efficiently distributes traffic flows by utilizing a dynamic re-routing algorithm that leverages real-time density information and the fundamental diagrams of each network edge. This re-routing approach predicts when each edge will reach critical density and proactively adjusts the routing algorithm's weights to prevent congestion before it occurs. The low-level controller coordinates CAVs as they cross signal-free intersections, generating optimal, fuel-efficient trajectories while ensuring safe passage by satisfying all relevant constraints. We formulate the problem as an optimal control problem and derive an analytical solution. Using the SUMO micro-simulation platform, we conduct simulation experiments on a realistic network. The results show that our hierarchical framework significantly enhances network performance compared to a baseline static routing approach. By dynamically re-routing vehicles, our approach successfully reduces total travel time and mitigates congestion before it develops.",
        "comments": ""
      },
      {
        "id": "2503.09158",
        "title": "FaVChat: Unlocking Fine-Grained Facial Video Understanding with Multimodal Large Language Models",
        "url": "http://arxiv.org/abs/2503.09158",
        "update_date": "2025-03-13",
        "first_author": "Fufangchen Zhao",
        "authors": "Fufangchen Zhao, Ming Li, Linrui Xu, Wenhao Jiang, Jian Gao, Danfeng Yan",
        "category": "cs.CV",
        "abstract": "Video-based multimodal large language models (VMLLMs) have demonstrated remarkable potential in cross-modal video understanding. However, their abilities in fine-grained face comprehension remain largely underexplored. Given its pivotal role in human-centric intelligence, developing VMLLMs for facial understanding holds a fundamental problem. To address this gap, we propose FaVChat, the first VMLLM specifically designed for fine-grained facial video understanding. To facilitate its training, we construct a large-scale facial video dataset comprising over 60k videos, with the majority annotated with 83 fine-grained facial attributes. These attributes are incorporated to enrich GPT-4o-generated captions, yielding 60k high-quality video-summary pairs and an additional 170k fine-grained question-answering (QA) pairs. To effectively capture rich facial clues, we propose a hybrid model architecture composed of a general visual encoder, a dedicated facial encoder, and a mixture-of-experts-enhanced adapter for adaptive fusion of multi-source visual features. To mitigate information loss during feature transformation, we extract multi-granularity representations from the facial encoder and integrate them into the subsequent LLM. This design enhances the model's ability to comprehend and respond to questions involving diverse levels of visual details. We employ a progressive training paradigm, transitioning from video summarization to a high-quality subset of video QA, gradually increasing task complexity to enhance the model's fine-grained visual perception. We conduct extensive zero-shot evaluation on a couple of public benchmarks, demonstrating that FaVChat consistently surpasses existing VMLLMs across multiple tasks.",
        "comments": ""
      }
    ],
    [
      {
        "id": "2503.09771",
        "title": "Estimating complete basis set extrapolation error through random walk",
        "url": "http://arxiv.org/abs/2503.09771",
        "update_date": "2025-03-12",
        "first_author": "Jakub Lang",
        "authors": "Jakub Lang, Michał Przybytek, Michał Lesiuk",
        "category": "physics.data-an",
        "abstract": "We propose a method of estimating the uncertainty of a result obtained through extrapolation to the complete basis set limit. The method is based on an ensemble of random walks which simulate all possible extrapolation outcomes that could have been obtained if results from larger basis sets had been available. The results assembled from a large collection of random walks can be then analyzed statistically, providing a route for uncertainty prediction at a confidence level required in a particular application. The method is free of empirical parameters and compatible with any extrapolation scheme. The proposed technique is tested in a series of numerical trials by comparing the determined confidence intervals with reliable reference data. We demonstrate that the predicted error bounds are reliable, tight, yet conservative at the same time.",
        "comments": ""
      },
      {
        "id": "2503.09716",
        "title": "MoE-Gen: High-Throughput MoE Inference on a Single GPU with Module-Based Batching",
        "url": "http://arxiv.org/abs/2503.09716",
        "update_date": "2025-03-12",
        "first_author": "Tairan Xu",
        "authors": "Tairan Xu, Leyang Xue, Zhan Lu, Adrian Jackson, Luo Mai",
        "category": "cs.DC",
        "abstract": "This paper presents MoE-Gen, a high-throughput MoE inference system optimized for single-GPU execution. Existing inference systems rely on model-based or continuous batching strategies, originally designed for interactive inference, which result in excessively small batches for MoE's key modules-attention and expert modules-leading to poor throughput. To address this, we introduce module-based batching, which accumulates tokens in host memory and dynamically launches large batches on GPUs to maximize utilization. Additionally, we optimize the choice of batch sizes for each module in an MoE to fully overlap GPU computation and communication, maximizing throughput. Evaluation demonstrates that MoE-Gen achieves 8-31x higher throughput compared to state-of-the-art systems employing model-based batching (FlexGen, MoE-Lightning, DeepSpeed), and offers even greater throughput improvements over continuous batching systems (e.g., vLLM and Ollama) on popular MoE models (DeepSeek and Mixtral) across offline inference tasks. MoE-Gen's source code is publicly available at https://github.com/EfficientMoE/MoE-Gen",
        "comments": ""
      },
      {
        "id": "2503.09700",
        "title": "Hardware-Efficient Entanglement Distillation Using Bosonic Systems",
        "url": "http://arxiv.org/abs/2503.09700",
        "update_date": "2025-03-12",
        "first_author": "Shoham Jacoby",
        "authors": "Shoham Jacoby, Rotem Arnon-Friedman, Serge Rosenblum",
        "category": "quant-ph",
        "abstract": "High-fidelity entanglement shared between distant quantum systems is an essential resource for quantum communication and computation. Entanglement distillation addresses this need by converting multiple noisy Bell pairs into fewer higher-fidelity pairs, using only local quantum operations and classical communication. However, this approach typically requires a substantial overhead in the number of qubits. To bypass this hurdle, we propose to leverage the high-dimensional Hilbert space of a single pair of bosonic systems to store a large amount of entanglement, replacing the need for multi-qubit systems. To distill entanglement in such a setup, we devise a new entanglement distillation protocol, tailored for bosonic systems. The protocol converts a highly-entangled noisy state between two bosonic systems into a lower-dimensional but high-fidelity entangled state, using only local bosonic operations. We show that our protocol significantly enhances the fidelity of the entangled state in the presence of naturally occurring loss and dephasing errors. Compared to methods relying on multiple Bell pairs, our scheme offers a more hardware-efficient strategy, providing a practical route toward the realization of entanglement distillation.",
        "comments": "21 pages, 11 figures, including supplementary information"
      },
      {
        "id": "2503.09684",
        "title": "The benefit of ignorance for traffic through a random congestible network",
        "url": "http://arxiv.org/abs/2503.09684",
        "update_date": "2025-03-12",
        "first_author": "Alican Saray",
        "authors": "Alican Saray, Calvin Pozderac, Ari Josephson, Brian Skinner",
        "category": "cond-mat.dis-nn",
        "abstract": "When traffic is routed through a network that is susceptible to congestion, the self-interested decisions made by individual users do not, in general, produce the optimal flow. This discrepancy is quantified by the so-called \"price of anarchy.\" Here we consider whether the traffic produced by self-interested users is made better or worse when users have uncertain knowledge about the cost functions of the links in the network, and we define a parallel concept that we call the \"price of ignorance.\" We introduce a simple model in which fast, congestible links and slow, incongestible links are mixed randomly in a large network and users plan their routes with finite uncertainty about which of the two cost functions describes each link. One of our key findings is that a small level of user ignorance universally improves traffic, regardless of the network composition. Further, there is an optimal level of ignorance which, in our model, causes the self-interested user behavior to coincide with the optimum. Many features of our model can be understood analytically, including the optimal level of user ignorance and the existence of critical scaling near the percolation threshold for fast links, where the potential benefit of user ignorance is greatest.",
        "comments": "8 + 2 pages, 5 figures"
      },
      {
        "id": "2503.09602",
        "title": "Odd-parity altermagnetism through sublattice currents: From Haldane-Hubbard model to general bipartite lattices",
        "url": "http://arxiv.org/abs/2503.09602",
        "update_date": "2025-03-12",
        "first_author": "Yu-Ping Lin",
        "authors": "Yu-Ping Lin",
        "category": "cond-mat.str-el",
        "abstract": "We propose the sublattice currents as a feasible route to odd-parity altermagnetism (ALM), where nonrelativistic collinear spin splitting occurs in the bands as an odd function of momentum. In contrast to previously classified ALMs, the sublattice currents break the time-reversal symmetry in nonmagnetic crystal structures and allow for such odd-parity spin splitting. A representative example is the Haldane-Hubbard model at half filling. Although the compensated collinear magnetic ground state was previously recognized as antiferromagnetism, we show that sublattice currents induce spin splitting in the bands and therefore turn it into an odd-parity ALM. Interestingly, its topological version serves as an example of ALM Chern insulator. We further generalize the Haldane-Hubbard model to common two- and three-dimensional bipartite lattices. With spin splitting from sublattice currents, the compensated collinear magnetic ground states at half filling are generally odd-parity ALM.",
        "comments": "6 pages, 3 figures"
      },
      {
        "id": "2503.09504",
        "title": "Double-Stage Feature-Level Clustering-Based Mixture of Experts Framework",
        "url": "http://arxiv.org/abs/2503.09504",
        "update_date": "2025-03-12",
        "first_author": "Bakary Badjie",
        "authors": "Bakary Badjie, José Cecílio, António Casimiro",
        "category": "cs.LG",
        "abstract": "The Mixture-of-Experts (MoE) model has succeeded in deep learning (DL). However, its complex architecture and advantages over dense models in image classification remain unclear. In previous studies, MoE performance has often been affected by noise and outliers in the input space. Some approaches incorporate input clustering for training MoE models, but most clustering algorithms lack access to labeled data, limiting their effectiveness. This paper introduces the Double-stage Feature-level Clustering and Pseudo-labeling-based Mixture of Experts (DFCP-MoE) framework, which consists of input feature extraction, feature-level clustering, and a computationally efficient pseudo-labeling strategy. This approach reduces the impact of noise and outliers while leveraging a small subset of labeled data to label a large portion of unlabeled inputs. We propose a conditional end-to-end joint training method that improves expert specialization by training the MoE model on well-labeled, clustered inputs. Unlike traditional MoE and dense models, the DFCP-MoE framework effectively captures input space diversity, leading to competitive inference results. We validate our approach on three benchmark datasets for multi-class classification tasks.",
        "comments": "14 Pages, 1 Figure, and 3 Tables"
      },
      {
        "id": "2503.09498",
        "title": "Towards Robust Multimodal Representation: A Unified Approach with Adaptive Experts and Alignment",
        "url": "http://arxiv.org/abs/2503.09498",
        "update_date": "2025-03-12",
        "first_author": "Nazanin Moradinasab",
        "authors": "Nazanin Moradinasab, Saurav Sengupta, Jiebei Liu, Sana Syed, Donald E. Brown",
        "category": "cs.LG",
        "abstract": "Healthcare relies on multiple types of data, such as medical images, genetic information, and clinical records, to improve diagnosis and treatment. However, missing data is a common challenge due to privacy restrictions, cost, and technical issues, making many existing multi-modal models unreliable. To address this, we propose a new multi-model model called Mixture of Experts, Symmetric Aligning, and Reconstruction (MoSARe), a deep learning framework that handles incomplete multimodal data while maintaining high accuracy. MoSARe integrates expert selection, cross-modal attention, and contrastive learning to improve feature representation and decision-making. Our results show that MoSARe outperforms existing models in situations when the data is complete. Furthermore, it provides reliable predictions even when some data are missing. This makes it especially useful in real-world healthcare settings, including resource-limited environments. Our code is publicly available at https://github.com/NazaninMn/MoSARe.",
        "comments": ""
      },
      {
        "id": "2503.09445",
        "title": "Astrea: A MOE-based Visual Understanding Model with Progressive Alignment",
        "url": "http://arxiv.org/abs/2503.09445",
        "update_date": "2025-03-12",
        "first_author": "Xiaoda Yang",
        "authors": "Xiaoda Yang, JunYu Lu, Hongshun Qiu, Sijing Li, Hao Li, Shengpeng Ji, Xudong Tang, Jiayang Xu, Jiaqi Duan, Ziyue Jiang, Cong Lin, Sihang Cai, Zejian Xie, Zhuoyang Song, Songxin Zhang",
        "category": "cs.CV",
        "abstract": "Vision-Language Models (VLMs) based on Mixture-of-Experts (MoE) architectures have emerged as a pivotal paradigm in multimodal understanding, offering a powerful framework for integrating visual and linguistic information. However, the increasing complexity and diversity of tasks present significant challenges in coordinating load balancing across heterogeneous visual experts, where optimizing one specialist's performance often compromises others' capabilities. To address task heterogeneity and expert load imbalance, we propose Astrea, a novel multi-expert collaborative VLM architecture based on progressive pre-alignment. Astrea introduces three key innovations: 1) A heterogeneous expert coordination mechanism that integrates four specialized models (detection, segmentation, classification, captioning) into a comprehensive expert matrix covering essential visual comprehension elements; 2) A dynamic knowledge fusion strategy featuring progressive pre-alignment to harmonize experts within the VLM latent space through contrastive learning, complemented by probabilistically activated stochastic residual connections to preserve knowledge continuity; 3) An enhanced optimization framework utilizing momentum contrastive learning for long-range dependency modeling and adaptive weight allocators for real-time expert contribution calibration. Extensive evaluations across 12 benchmark tasks spanning VQA, image captioning, and cross-modal retrieval demonstrate Astrea's superiority over state-of-the-art models, achieving an average performance gain of +4.7\\%. This study provides the first empirical demonstration that progressive pre-alignment strategies enable VLMs to overcome task heterogeneity limitations, establishing new methodological foundations for developing general-purpose multimodal agents.",
        "comments": ""
      },
      {
        "id": "2503.09357",
        "title": "Automatic Operator-level Parallelism Planning for Distributed Deep Learning -- A Mixed-Integer Programming Approach",
        "url": "http://arxiv.org/abs/2503.09357",
        "update_date": "2025-03-12",
        "first_author": "Ruifeng She",
        "authors": "Ruifeng She, Bowen Pang, Kai Li, Zehua Liu, Tao Zhong",
        "category": "cs.LG",
        "abstract": "As the artificial intelligence community advances into the era of large models with billions of parameters, distributed training and inference have become essential. While various parallelism strategies-data, model, sequence, and pipeline-have been successfully implemented for popular neural networks on main-stream hardware, optimizing the distributed deployment schedule requires extensive expertise and manual effort. Further more, while existing frameworks with most simple chain-like structures, they struggle with complex non-linear architectures. Mixture-of-experts and multi-modal models feature intricate MIMO and branch-rich topologies that require fine-grained operator-level parallelization beyond the capabilities of existing frameworks. We propose formulating parallelism planning as a scheduling optimization problem using mixed-integer programming. We propose a bi-level solution framework balancing optimality with computational efficiency, automatically generating effective distributed plans that capture both the heterogeneous structure of modern neural networks and the underlying hardware constraints. In experiments comparing against expert-designed strategies like DeepSeek's DualPipe, our framework achieves comparable or superior performance, reducing computational bubbles by half under the same memory constraints. The framework's versatility extends beyond throughput optimization to incorporate hardware utilization maximization, memory capacity constraints, and other considerations or potential strategies. Such capabilities position our solution as both a valuable research tool for exploring optimal parallelization strategies and a practical industrial solution for large-scale AI deployment.",
        "comments": ""
      },
      {
        "id": "2503.09304",
        "title": "Priority-Aware Preemptive Scheduling for Mixed-Priority Workloads in MoE Inference",
        "url": "http://arxiv.org/abs/2503.09304",
        "update_date": "2025-03-12",
        "first_author": "Mohammad Siavashi",
        "authors": "Mohammad Siavashi, Faezeh Keshmiri Dindarloo, Dejan Kostic, Marco Chiesa",
        "category": "cs.LG",
        "abstract": "Large Language Models have revolutionized natural language processing, yet serving them efficiently in data centers remains challenging due to mixed workloads comprising latency-sensitive (LS) and best-effort (BE) jobs. Existing inference systems employ iteration-level first-come-first-served scheduling, causing head-of-line blocking when BE jobs delay LS jobs. We introduce QLLM, a novel inference system designed for Mixture of Experts (MoE) models, featuring a fine-grained, priority-aware preemptive scheduler. QLLM enables expert-level preemption, deferring BE job execution while minimizing LS time-to-first-token (TTFT). Our approach removes iteration-level scheduling constraints, enabling the scheduler to preempt jobs at any layer based on priority. Evaluations on an Nvidia A100 GPU show that QLLM significantly improves performance. It reduces LS TTFT by an average of $65.5\\times$ and meets the SLO at up to $7$ requests/sec, whereas the baseline fails to do so under the tested workload. Additionally, it cuts LS turnaround time by up to $12.8\\times$ without impacting throughput. QLLM is modular, extensible, and seamlessly integrates with Hugging Face MoE models.",
        "comments": ""
      }
    ]
  ],
  "LLM Fine-tuning": [
    [
      {
        "id": "2503.10637",
        "title": "Distilling Diversity and Control in Diffusion Models",
        "url": "http://arxiv.org/abs/2503.10637",
        "update_date": "2025-03-13",
        "first_author": "Rohit Gandikota",
        "authors": "Rohit Gandikota, David Bau",
        "category": "cs.GR",
        "abstract": "Distilled diffusion models suffer from a critical limitation: reduced sample diversity compared to their base counterparts. In this work, we uncover that despite this diversity loss, distilled models retain the fundamental concept representations of base models. We demonstrate control distillation - where control mechanisms like Concept Sliders and LoRAs trained on base models can be seamlessly transferred to distilled models and vice-versa, effectively distilling control without any retraining. This preservation of representational structure prompted our investigation into the mechanisms of diversity collapse during distillation. To understand how distillation affects diversity, we introduce Diffusion Target (DT) Visualization, an analysis and debugging tool that reveals how models predict final outputs at intermediate steps. Through DT-Visualization, we identify generation artifacts, inconsistencies, and demonstrate that initial diffusion timesteps disproportionately determine output diversity, while later steps primarily refine details. Based on these insights, we introduce diversity distillation - a hybrid inference approach that strategically employs the base model for only the first critical timestep before transitioning to the efficient distilled model. Our experiments demonstrate that this simple modification not only restores the diversity capabilities from base to distilled models but surprisingly exceeds it, while maintaining nearly the computational efficiency of distilled inference, all without requiring additional training or model modifications. Our code and data are available at https://distillation.baulab.info",
        "comments": "Project Page: https://distillation.baulab.info"
      },
      {
        "id": "2503.10631",
        "title": "HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model",
        "url": "http://arxiv.org/abs/2503.10631",
        "update_date": "2025-03-13",
        "first_author": "Jiaming Liu",
        "authors": "Jiaming Liu, Hao Chen, Pengju An, Zhuoyang Liu, Renrui Zhang, Chenyang Gu, Xiaoqi Li, Ziyu Guo, Sixiang Chen, Mengzhen Liu, Chengkai Hou, Mengdi Zhao, KC alex Zhou, Pheng-Ann Heng, Shanghang Zhang",
        "category": "cs.CV",
        "abstract": "Recent advancements in vision-language models (VLMs) for common-sense reasoning have led to the development of vision-language-action (VLA) models, enabling robots to perform generalized manipulation. Although existing autoregressive VLA methods leverage large-scale pretrained knowledge, they disrupt the continuity of actions. Meanwhile, some VLA methods incorporate an additional diffusion head to predict continuous actions, relying solely on VLM-extracted features, which limits their reasoning capabilities. In this paper, we introduce HybridVLA, a unified framework that seamlessly integrates the strengths of both autoregressive and diffusion policies within a single large language model, rather than simply connecting them. To bridge the generation gap, a collaborative training recipe is proposed that injects the diffusion modeling directly into the next-token prediction. With this recipe, we find that these two forms of action prediction not only reinforce each other but also exhibit varying performance across different tasks. Therefore, we design a collaborative action ensemble mechanism that adaptively fuses these two predictions, leading to more robust control. In experiments, HybridVLA outperforms previous state-of-the-art VLA methods across various simulation and real-world tasks, including both single-arm and dual-arm robots, while demonstrating stable manipulation in previously unseen configurations.",
        "comments": ""
      },
      {
        "id": "2503.10620",
        "title": "From TOWER to SPIRE: Adding the Speech Modality to a Text-Only LLM",
        "url": "http://arxiv.org/abs/2503.10620",
        "update_date": "2025-03-13",
        "first_author": "Kshitij Ambilduke",
        "authors": "Kshitij Ambilduke, Ben Peters, Sonal Sannigrahi, Anil Keshwani, Tsz Kin Lam, Bruno Martins, Marcely Zanon Boito, André F. T. Martins",
        "category": "cs.CL",
        "abstract": "Large language models (LLMs) have shown remarkable performance and generalization capabilities across multiple languages and tasks, making them very attractive targets for multi-modality integration (e.g., images or speech). In this work, we extend an existing LLM to the speech modality via speech discretization and continued pre-training. In particular, we are interested in multilingual LLMs, such as TOWER, as their pre-training setting allows us to treat discretized speech input as an additional translation language. The resulting open-source model, SPIRE, is able to transcribe and translate English speech input while maintaining TOWER's original performance on translation-related tasks, showcasing that discretized speech input integration as an additional language is feasible during LLM adaptation. We make our code and models available to the community.",
        "comments": ""
      },
      {
        "id": "2503.10617",
        "title": "Compositional Subspace Representation Fine-tuning for Adaptive Large Language Models",
        "url": "http://arxiv.org/abs/2503.10617",
        "update_date": "2025-03-13",
        "first_author": "Andy Zhou",
        "authors": "Andy Zhou",
        "category": "cs.CL",
        "abstract": "Adapting large language models to multiple tasks can cause cross-skill interference, where improvements for one skill degrade another. While methods such as LoRA impose orthogonality constraints at the weight level, they do not fully address interference in hidden-state representations. We propose Compositional Subspace Representation Fine-tuning (CS-ReFT), a novel representation-based approach that learns multiple orthonormal subspace transformations, each specializing in a distinct skill, and composes them via a lightweight router. By isolating these subspace edits in the hidden state, rather than weight matrices, CS-ReFT prevents cross-task conflicts more effectively. On the AlpacaEval benchmark, applying CS-ReFT to Llama-2-7B achieves a 93.94% win rate, surpassing GPT-3.5 Turbo (86.30%) while requiring only 0.0098% of model parameters. These findings show that specialized representation edits, composed via a simple router, significantly enhance multi-task instruction following with minimal overhead.",
        "comments": "Accepted to ICLR 2025 SCOPE"
      },
      {
        "id": "2503.10616",
        "title": "OVTR: End-to-End Open-Vocabulary Multiple Object Tracking with Transformer",
        "url": "http://arxiv.org/abs/2503.10616",
        "update_date": "2025-03-13",
        "first_author": "Jinyang Li",
        "authors": "Jinyang Li, En Yu, Sijia Chen, Wenbing Tao",
        "category": "cs.CV",
        "abstract": "Open-vocabulary multiple object tracking aims to generalize trackers to unseen categories during training, enabling their application across a variety of real-world scenarios. However, the existing open-vocabulary tracker is constrained by its framework structure, isolated frame-level perception, and insufficient modal interactions, which hinder its performance in open-vocabulary classification and tracking. In this paper, we propose OVTR (End-to-End Open-Vocabulary Multiple Object Tracking with TRansformer), the first end-to-end open-vocabulary tracker that models motion, appearance, and category simultaneously. To achieve stable classification and continuous tracking, we design the CIP (Category Information Propagation) strategy, which establishes multiple high-level category information priors for subsequent frames. Additionally, we introduce a dual-branch structure for generalization capability and deep multimodal interaction, and incorporate protective strategies in the decoder to enhance performance. Experimental results show that our method surpasses previous trackers on the open-vocabulary MOT benchmark while also achieving faster inference speeds and significantly reducing preprocessing requirements. Moreover, the experiment transferring the model to another dataset demonstrates its strong adaptability. Models and code are released at https://github.com/jinyanglii/OVTR.",
        "comments": "Accepted by ICLR 2025"
      },
      {
        "id": "2503.10614",
        "title": "ConsisLoRA: Enhancing Content and Style Consistency for LoRA-based Style Transfer",
        "url": "http://arxiv.org/abs/2503.10614",
        "update_date": "2025-03-13",
        "first_author": "Bolin Chen",
        "authors": "Bolin Chen, Baoquan Zhao, Haoran Xie, Yi Cai, Qing Li, Xudong Mao",
        "category": "cs.CV",
        "abstract": "Style transfer involves transferring the style from a reference image to the content of a target image. Recent advancements in LoRA-based (Low-Rank Adaptation) methods have shown promise in effectively capturing the style of a single image. However, these approaches still face significant challenges such as content inconsistency, style misalignment, and content leakage. In this paper, we comprehensively analyze the limitations of the standard diffusion parameterization, which learns to predict noise, in the context of style transfer. To address these issues, we introduce ConsisLoRA, a LoRA-based method that enhances both content and style consistency by optimizing the LoRA weights to predict the original image rather than noise. We also propose a two-step training strategy that decouples the learning of content and style from the reference image. To effectively capture both the global structure and local details of the content image, we introduce a stepwise loss transition strategy. Additionally, we present an inference guidance method that enables continuous control over content and style strengths during inference. Through both qualitative and quantitative evaluations, our method demonstrates significant improvements in content and style consistency while effectively reducing content leakage.",
        "comments": ""
      },
      {
        "id": "2503.10605",
        "title": "OCCUQ: Exploring Efficient Uncertainty Quantification for 3D Occupancy Prediction",
        "url": "http://arxiv.org/abs/2503.10605",
        "update_date": "2025-03-13",
        "first_author": "Severin Heidrich",
        "authors": "Severin Heidrich, Till Beemelmanns, Alexey Nekrasov, Bastian Leibe, Lutz Eckstein",
        "category": "cs.CV",
        "abstract": "Autonomous driving has the potential to significantly enhance productivity and provide numerous societal benefits. Ensuring robustness in these safety-critical systems is essential, particularly when vehicles must navigate adverse weather conditions and sensor corruptions that may not have been encountered during training. Current methods often overlook uncertainties arising from adversarial conditions or distributional shifts, limiting their real-world applicability. We propose an efficient adaptation of an uncertainty estimation technique for 3D occupancy prediction. Our method dynamically calibrates model confidence using epistemic uncertainty estimates. Our evaluation under various camera corruption scenarios, such as fog or missing cameras, demonstrates that our approach effectively quantifies epistemic uncertainty by assigning higher uncertainty values to unseen data. We introduce region-specific corruptions to simulate defects affecting only a single camera and validate our findings through both scene-level and region-level assessments. Our results show superior performance in Out-of-Distribution (OoD) detection and confidence calibration compared to common baselines such as Deep Ensembles and MC-Dropout. Our approach consistently demonstrates reliable uncertainty measures, indicating its potential for enhancing the robustness of autonomous driving systems in real-world scenarios. Code and dataset are available at https://github.com/ika-rwth-aachen/OCCUQ .",
        "comments": "Accepted for publication at ICRA 2025"
      },
      {
        "id": "2503.10587",
        "title": "The Spectral Bias of Shallow Neural Network Learning is Shaped by the Choice of Non-linearity",
        "url": "http://arxiv.org/abs/2503.10587",
        "update_date": "2025-03-13",
        "first_author": "Justin Sahs",
        "authors": "Justin Sahs, Ryan Pyle, Fabio Anselmi, Ankit Patel",
        "category": "cs.LG",
        "abstract": "Despite classical statistical theory predicting severe overfitting, modern massively overparameterized neural networks still generalize well. This unexpected property is attributed to the network's so-called implicit bias, which describes its propensity to converge to solutions that generalize effectively, among the many possible that correctly label the training data. The aim of our research is to explore this bias from a new perspective, focusing on how non-linear activation functions contribute to shaping it. First, we introduce a reparameterization which removes a continuous weight rescaling symmetry. Second, in the kernel regime, we leverage this reparameterization to generalize recent findings that relate shallow Neural Networks to the Radon transform, deriving an explicit formula for the implicit bias induced by a broad class of activation functions. Specifically, by utilizing the connection between the Radon transform and the Fourier transform, we interpret the kernel regime's inductive bias as minimizing a spectral seminorm that penalizes high-frequency components, in a manner dependent on the activation function. Finally, in the adaptive regime, we demonstrate the existence of local dynamical attractors that facilitate the formation of clusters of hyperplanes where the input to a neuron's activation function is zero, yielding alignment between many neurons' response functions. We confirm these theoretical results with simulations. All together, our work provides a deeper understanding of the mechanisms underlying the generalization capabilities of overparameterized neural networks and its relation with the implicit bias, offering potential pathways for designing more efficient and robust models.",
        "comments": "18 pages, 10 figures in main text"
      },
      {
        "id": "2503.10562",
        "title": "Discontinuous Galerkin discretization of conservative dynamical low-rank approximation schemes for the Vlasov-Poisson equation",
        "url": "http://arxiv.org/abs/2503.10562",
        "update_date": "2025-03-13",
        "first_author": "André Uschmajew",
        "authors": "André Uschmajew, Andreas Zeiser",
        "category": "math.NA",
        "abstract": "A numerical dynamical low-rank approximation (DLRA) scheme for the solution of the Vlasov-Poisson equation is presented. Based on the formulation of the DLRA equations as Friedrichs' systems in a continuous setting, it combines recently proposed conservative DLRA methods with a discontinuous Galerkin discretization. The resulting scheme is shown to ensure mass and momentum conservation at the discrete level. In addition, a new formulation of the conservative integrator is proposed which facilitates a projector splitting integrator. Numerical experiments validate our approach in one- and two-dimensional simulations of Landau damping. As a demonstration of feasibility, it is also shown that the rank-adaptive unconventional integrator can be combined with mesh adaptivity.",
        "comments": ""
      },
      {
        "id": "2503.10559",
        "title": "Towards Safe Path Tracking Using the Simplex Architecture",
        "url": "http://arxiv.org/abs/2503.10559",
        "update_date": "2025-03-13",
        "first_author": "Georg Jäger",
        "authors": "Georg Jäger, Nils-Jonathan Friedrich, Hauke Petersen, Benjamin Noack",
        "category": "cs.RO",
        "abstract": "Robot navigation in complex environments necessitates controllers that are adaptive and safe. Traditional controllers like Regulated Pure Pursuit, Dynamic Window Approach, and Model-Predictive Path Integral, while reliable, struggle to adapt to dynamic conditions. Reinforcement Learning offers adaptability but lacks formal safety guarantees. To address this, we propose a path tracking controller leveraging the Simplex architecture. It combines a Reinforcement Learning controller for adaptiveness and performance with a high-assurance controller providing safety and stability. Our contribution is twofold. We firstly discuss general stability and safety considerations for designing controllers using the Simplex architecture. Secondly, we present a Simplex-based path tracking controller. Our simulation results, supported by preliminary in-field tests, demonstrate the controller's effectiveness in maintaining safety while achieving comparable performance to state-of-the-art methods.",
        "comments": ""
      }
    ],
    [
      {
        "id": "2503.10554",
        "title": "NuExo: A Wearable Exoskeleton Covering all Upper Limb ROM for Outdoor Data Collection and Teleoperation of Humanoid Robots",
        "url": "http://arxiv.org/abs/2503.10554",
        "update_date": "2025-03-13",
        "first_author": "Rui Zhong",
        "authors": "Rui Zhong, Chuang Cheng, Junpeng Xu, Yantong Wei, Ce Guo, Daoxun Zhang, Wei Dai, Huimin Lu",
        "category": "cs.RO",
        "abstract": "The evolution from motion capture and teleoperation to robot skill learning has emerged as a hotspot and critical pathway for advancing embodied intelligence. However, existing systems still face a persistent gap in simultaneously achieving four objectives: accurate tracking of full upper limb movements over extended durations (Accuracy), ergonomic adaptation to human biomechanics (Comfort), versatile data collection (e.g., force data) and compatibility with humanoid robots (Versatility), and lightweight design for outdoor daily use (Convenience). We present a wearable exoskeleton system, incorporating user-friendly immersive teleoperation and multi-modal sensing collection to bridge this gap. Due to the features of a novel shoulder mechanism with synchronized linkage and timing belt transmission, this system can adapt well to compound shoulder movements and replicate 100% coverage of natural upper limb motion ranges. Weighing 5.2 kg, NuExo supports backpack-type use and can be conveniently applied in daily outdoor scenarios. Furthermore, we develop a unified intuitive teleoperation framework and a comprehensive data collection system integrating multi-modal sensing for various humanoid robots. Experiments across distinct humanoid platforms and different users validate our exoskeleton's superiority in motion range and flexibility, while confirming its stability in data collection and teleoperation accuracy in dynamic scenarios.",
        "comments": "8 pages"
      },
      {
        "id": "2503.10549",
        "title": "MASQUE: A Text-Guided Diffusion-Based Framework for Localized and Customized Adversarial Makeup",
        "url": "http://arxiv.org/abs/2503.10549",
        "update_date": "2025-03-13",
        "first_author": "Youngjin Kwon",
        "authors": "Youngjin Kwon, Xiao Zhang",
        "category": "cs.CV",
        "abstract": "As facial recognition is increasingly adopted for government and commercial services, its potential misuse has raised serious concerns about privacy and civil rights. To counteract, various anti-facial recognition techniques have been proposed for privacy protection by adversarially perturbing face images, among which generative makeup-based approaches are the most popular. However, these methods, designed primarily to impersonate specific target identities, can only achieve weak dodging success rates while increasing the risk of targeted abuse. In addition, they often introduce global visual artifacts or a lack of adaptability to accommodate diverse makeup prompts, compromising user satisfaction. To address the above limitations, we develop MASQUE, a novel diffusion-based framework that generates localized adversarial makeups guided by user-defined text prompts. Built upon precise null-text inversion, customized cross-attention fusion with masking, and a pairwise adversarial guidance mechanism using images of the same individual, MASQUE achieves robust dodging performance without requiring any external identity. Comprehensive evaluations on open-source facial recognition models and commercial APIs demonstrate that MASQUE significantly improves dodging success rates over all baselines, along with higher perceptual fidelity and stronger adaptability to various text makeup prompts.",
        "comments": ""
      },
      {
        "id": "2503.10544",
        "title": "DP-GPL: Differentially Private Graph Prompt Learning",
        "url": "http://arxiv.org/abs/2503.10544",
        "update_date": "2025-03-13",
        "first_author": "Jing Xu",
        "authors": "Jing Xu, Franziska Boenisch, Iyiola Emmanuel Olatunji, Adam Dziedzic",
        "category": "cs.LG",
        "abstract": "Graph Neural Networks (GNNs) have shown remarkable performance in various applications. Recently, graph prompt learning has emerged as a powerful GNN training paradigm, inspired by advances in language and vision foundation models. Here, a GNN is pre-trained on public data and then adapted to sensitive tasks using lightweight graph prompts. However, using prompts from sensitive data poses privacy risks. In this work, we are the first to investigate these practical risks in graph prompts by instantiating a membership inference attack that reveals significant privacy leakage. We also find that the standard privacy method, DP-SGD, fails to provide practical privacy-utility trade-offs in graph prompt learning, likely due to the small number of sensitive data points used to learn the prompts. As a solution, we propose DP-GPL for differentially private graph prompt learning based on the PATE framework, that generates a graph prompt with differential privacy guarantees. Our evaluation across various graph prompt learning methods, GNN architectures, and pre-training strategies demonstrates that our algorithm achieves high utility at strong privacy, effectively mitigating privacy concerns while preserving the powerful capabilities of prompted GNNs as powerful foundation models in the graph domain.",
        "comments": ""
      },
      {
        "id": "2503.10537",
        "title": "Structured Preconditioners in Adaptive Optimization: A Unified Analysis",
        "url": "http://arxiv.org/abs/2503.10537",
        "update_date": "2025-03-13",
        "first_author": "Shuo Xie",
        "authors": "Shuo Xie, Tianhao Wang, Sashank Reddi, Sanjiv Kumar, Zhiyuan Li",
        "category": "cs.LG",
        "abstract": "We present a novel unified analysis for a broad class of adaptive optimization algorithms with structured (e.g., layerwise, diagonal, and kronecker-factored) preconditioners for both online regret minimization and offline convex optimization. Our analysis not only provides matching rate to several important structured preconditioned algorithms including diagonal AdaGrad, full-matrix AdaGrad, and AdaGrad-Norm, but also gives an improved convergence rate for a one-sided variant of Shampoo over that of original Shampoo. Interestingly, more structured preconditioners (e.g., diagonal Adagrad, AdaGrad-Norm which use less space and compute) are often presented as computationally efficient approximations to full-matrix Adagrad, aiming for improved optimization performance through better approximations. Our unified analysis challenges this prevailing view and reveals, perhaps surprisingly, that more structured preconditioners, despite using less space and computation per step, can outperform their less structured counterparts. To demonstrate this, we show that one-sided Shampoo, which is relatively much cheaper than full-matrix AdaGrad could outperform it both theoretically and experimentally.",
        "comments": ""
      },
      {
        "id": "2503.10526",
        "title": "NeighborRetr: Balancing Hub Centrality in Cross-Modal Retrieval",
        "url": "http://arxiv.org/abs/2503.10526",
        "update_date": "2025-03-13",
        "first_author": "Zengrong Lin",
        "authors": "Zengrong Lin, Zheng Wang, Tianwen Qian, Pan Mu, Sixian Chan, Cong Bai",
        "category": "cs.CV",
        "abstract": "Cross-modal retrieval aims to bridge the semantic gap between different modalities, such as visual and textual data, enabling accurate retrieval across them. Despite significant advancements with models like CLIP that align cross-modal representations, a persistent challenge remains: the hubness problem, where a small subset of samples (hubs) dominate as nearest neighbors, leading to biased representations and degraded retrieval accuracy. Existing methods often mitigate hubness through post-hoc normalization techniques, relying on prior data distributions that may not be practical in real-world scenarios. In this paper, we directly mitigate hubness during training and introduce NeighborRetr, a novel method that effectively balances the learning of hubs and adaptively adjusts the relations of various kinds of neighbors. Our approach not only mitigates the hubness problem but also enhances retrieval performance, achieving state-of-the-art results on multiple cross-modal retrieval benchmarks. Furthermore, NeighborRetr demonstrates robust generalization to new domains with substantial distribution shifts, highlighting its effectiveness in real-world applications. We make our code publicly available at: https://github.com/zzezze/NeighborRetr .",
        "comments": "Accepted at CVPR 2025, 18 pages, 7 figures, 13 tables"
      },
      {
        "id": "2503.10508",
        "title": "Hoi2Anomaly: An Explainable Anomaly Detection Approach Guided by Human-Object Interaction",
        "url": "http://arxiv.org/abs/2503.10508",
        "update_date": "2025-03-13",
        "first_author": "Yuhan Wang",
        "authors": "Yuhan Wang, Cheng Liu, Daou Zhang, Weichao Wu",
        "category": "cs.CV",
        "abstract": "In the domain of Image Anomaly Detection (IAD), Existing methods frequently exhibit a paucity of fine-grained, interpretable semantic information, resulting in the detection of anomalous entities or activities that are susceptible to machine illusions. This deficiency often leads to the detection of anomalous entities or actions that are susceptible to machine illusions and lack sufficient explanation. In this thesis, we propose a novel approach to anomaly detection, termed Hoi2Anomaly, which aims to achieve precise discrimination and localization of anomalies. The proposed methodology involves the construction of a multi-modal instruction tuning dataset comprising human-object interaction (HOI) pairs in anomalous scenarios. Second, we have trained an HOI extractor in threat scenarios to localize and match anomalous actions and entities. Finally, explanatory content is generated for the detected anomalous HOI by fine-tuning the visual language pretraining (VLP) framework. The experimental results demonstrate that Hoi2Anomaly surpasses existing generative approaches in terms of precision and explainability. We will release Hoi2Anomaly for the advancement of the field of anomaly detection.",
        "comments": ""
      },
      {
        "id": "2503.10503",
        "title": "Sample Compression for Continual Learning",
        "url": "http://arxiv.org/abs/2503.10503",
        "update_date": "2025-03-13",
        "first_author": "Jacob Comeau",
        "authors": "Jacob Comeau, Mathieu Bazinet, Pascal Germain, Cem Subakan",
        "category": "cs.LG",
        "abstract": "Continual learning algorithms aim to learn from a sequence of tasks, making the training distribution non-stationary. The majority of existing continual learning approaches in the literature rely on heuristics and do not provide learning guarantees for the continual learning setup. In this paper, we present a new method called 'Continual Pick-to-Learn' (CoP2L), which is able to retain the most representative samples for each task in an efficient way. The algorithm is adapted from the Pick-to-Learn algorithm, rooted in the sample compression theory. This allows us to provide high-confidence upper bounds on the generalization loss of the learned predictors, numerically computable after every update of the learned model. We also empirically show on several standard continual learning benchmarks that our algorithm is able to outperform standard experience replay, significantly mitigating catastrophic forgetting.",
        "comments": ""
      },
      {
        "id": "2503.10492",
        "title": "Meta-learning characteristics and dynamics of quantum systems",
        "url": "http://arxiv.org/abs/2503.10492",
        "update_date": "2025-03-13",
        "first_author": "Lucas Schorling",
        "authors": "Lucas Schorling, Pranav Vaidhyanathan, Jonas Schuff, Miguel J. Carballido, Dominik Zumbühl, Gerard Milburn, Florian Marquardt, Jakob Foerster, Michael A. Osborne, Natalia Ares",
        "category": "quant-ph",
        "abstract": "While machine learning holds great promise for quantum technologies, most current methods focus on predicting or controlling a specific quantum system. Meta-learning approaches, however, can adapt to new systems for which little data is available, by leveraging knowledge obtained from previous data associated with similar systems. In this paper, we meta-learn dynamics and characteristics of closed and open two-level systems, as well as the Heisenberg model. Based on experimental data of a Loss-DiVincenzo spin-qubit hosted in a Ge/Si core/shell nanowire for different gate voltage configurations, we predict qubit characteristics i.e. $g$-factor and Rabi frequency using meta-learning. The algorithm we introduce improves upon previous state-of-the-art meta-learning methods for physics-based systems by introducing novel techniques such as adaptive learning rates and a global optimizer for improved robustness and increased computational efficiency. We benchmark our method against other meta-learning methods, a vanilla transformer, and a multilayer perceptron, and demonstrate improved performance.",
        "comments": "6+1 pages, 4 figures. L. Schorling and P. Vaidhyanathan contributed\n  equally to this work"
      },
      {
        "id": "2503.10478",
        "title": "Multiscale simulation of interacting turbulent and rarefied gas flows in the DSMC framework",
        "url": "http://arxiv.org/abs/2503.10478",
        "update_date": "2025-03-13",
        "first_author": "Liyan Luo",
        "authors": "Liyan Luo, Songyan Tian, Lei Wu",
        "category": "physics.comp-ph",
        "abstract": "A multiscale stochastic-deterministic coupling method is proposed to investigate the complex interactions between turbulent and rarefied gas flows within a unified framework. This method intermittently integrates the general synthetic iterative scheme with the shear stress transport turbulence model into the direct simulation Monte Carlo (DSMC) approach, enabling the simulation of gas flows across the free-molecular, transition, slip, and turbulent regimes. First, the macroscopic synthetic equations, derived directly from DSMC, are coupled with the turbulence model to establish a constitutive relation that incorporates not only turbulent and laminar transport coefficients but also higher-order terms accounting for rarefaction effects. Second, the macroscopic properties, statistically sampled over specific time intervals in DSMC, along with the turbulent properties provided by the turbulence model, serve as initial conditions for solving the macroscopic synthetic equations. Finally, the simulation particles in DSMC are updated based on the macroscopic properties obtained from the synthetic equations. Numerical simulations demonstrate that the proposed method asymptotically converges to either the turbulence model or DSMC results, adaptively adjusting to different flow regimes. Then, this coupling method is applied to simulate an opposing jet surrounded by hypersonic rarefied gas flows, revealing significant variations in surface properties due to the interplay of turbulent and rarefied effects. This study presents an efficient methodology for simulating the complex interplay between rarefied and turbulent flows, establishing a foundational framework for investigating the coupled effects of turbulence, hypersonic conditions, and chemical reactions in rarefied gas dynamics in the future.",
        "comments": ""
      },
      {
        "id": "2503.10460",
        "title": "Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and Beyond",
        "url": "http://arxiv.org/abs/2503.10460",
        "update_date": "2025-03-13",
        "first_author": "Liang Wen",
        "authors": "Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, Xiangzheng Zhang",
        "category": "cs.CL",
        "abstract": "This paper presents our work on the Light-R1 series, with models, data, and code all released.   We first focus on training long COT models from scratch, specifically starting from models initially lacking long COT capabilities. Using a curriculum training recipe consisting of two-stage SFT and semi-on-policy DPO, we train our model Light-R1-32B from Qwen2.5-32B-Instruct, resulting in superior math performance compared to DeepSeek-R1-Distill-Qwen-32B. Despite being trained exclusively on math data, Light-R1-32B shows strong generalization across other domains. In the subsequent phase of this work, we highlight the significant benefit of the 3k dataset constructed for the second SFT stage on enhancing other models. By fine-tuning DeepSeek-R1-Distilled models using this dataset, we obtain new SOTA models in 7B and 14B, while the 32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.   Furthermore, we extend our work by applying reinforcement learning, specifically GRPO, on long-COT models to further improve reasoning performance. We successfully train our final Light-R1-14B-DS with RL, achieving SOTA performance among 14B parameter models in math. With AIME24 & 25 scores of 74.0 and 60.2 respectively, Light-R1-14B-DS surpasses even many 32B models and DeepSeek-R1-Distill-Llama-70B. Its RL training also exhibits well expected behavior, showing simultaneous increase in response length and reward score.   The Light-R1 series of work validates training long-COT models from scratch, showcases the art in SFT data and releases SOTA models from RL.",
        "comments": "all release at https://github.com/Qihoo360/Light-R1"
      }
    ]
  ]
}